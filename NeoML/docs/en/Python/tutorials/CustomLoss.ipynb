{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "administrative-browse",
   "metadata": {},
   "source": [
    "Copyright Â© 2017-2021 ABBYY Production LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crude-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-birmingham",
   "metadata": {},
   "source": [
    "# Neural network with custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-mercy",
   "metadata": {},
   "source": [
    "[Download the tutorial as a Jupyter notebook](https://github.com/neoml-lib/neoml/blob/master/NeoML/docs/en/Python/tutorials/CustomLoss.ipynb)\n",
    "\n",
    "In this tutorial we will use NeoML to train a network that uses a custom loss function. User-defined loss functions have to be constructed out of operations supported in our autodifferentiation module.\n",
    "\n",
    "The tutorial includes the following steps:\n",
    "\n",
    "* [Build the network](#Build-the-network)\n",
    "* [Create custom loss](#Create-custom-loss)\n",
    "* [Train the network](#Train-the-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-mortgage",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-kuwait",
   "metadata": {},
   "source": [
    "We'll fix the random seed and use a single-thread CPU math engine to make experiments more precise and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "frozen-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neoml\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(666)\n",
    "math_engine = neoml.MathEngine.CpuMathEngine(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-payment",
   "metadata": {},
   "source": [
    "The network architecture will be:\n",
    "\n",
    "- `Source` for data\n",
    "- `FullyConnected` with 1024 elements\n",
    "- `Tanh` activation\n",
    "- `FullyConnected` with 1 element (for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "national-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = neoml.Dnn.Dnn(math_engine)\n",
    "data = neoml.Dnn.Source(dnn, name='data')\n",
    "fc1 = neoml.Dnn.FullyConnected(data, 1024, name='fc1')\n",
    "tanh = neoml.Dnn.Tanh(fc1, name='tanh')\n",
    "fc2 = neoml.Dnn.FullyConnected(tanh, 1, name='fc2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-subdivision",
   "metadata": {},
   "source": [
    "## Create custom loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-november",
   "metadata": {},
   "source": [
    "To use a custom loss function with NeoML, you need to implement the class that will calculate the function, derived from `neoml.Dnn.CustomLossCalculatorBase`. Implement the abstract `calc(self, data, labels)` method of this class. Its input parameters are `data` and `labels`, float blobs of size `(batchSize, 1, 1, 1, 1, 1, vectorSize)`, containing the network response and correct labels respectively.\n",
    "\n",
    "The `calc` method must return a blob of size `(batchSize, 1, 1, 1, 1, 1, 1)` with loss function values for each object in batch. Object weights processing, total loss calculation and gradient calculation will be done automatically afterwards.\n",
    "\n",
    "The following functions may be used in your custom losses:\n",
    "\n",
    "- `/ * + -` operations between blobs and floats\n",
    "- `neoml.AutoDiff.*` functions like `neoml.AutoDiff.max`, `neoml.AutoDiff.top_k` etc.\n",
    "- `neoml.AutoDiff.const` for creating additional blobs with given values\n",
    "\n",
    "In this example we'll implement hinge loss for binary clasification, just for the sake of demonstration (in fact, NeoML already provides a `HingeLoss` layer with this loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thick-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HingeLossCalculator(neoml.Dnn.CustomLossCalculatorBase):\n",
    "    def calc(self, data, labels):\n",
    "        # data contains net outputs (float, [batchSize, 1, 1, 1, 1, 1, 1])\n",
    "        # label contains correct answers (float, [batchSize, 1, 1, 1, 1, 1], +/-1)\n",
    "        # (vectorSize from above is equal to 1 because it's a binary classification loss)\n",
    "        # the formula is max(0, 1 - y * t) where t is a correct label and y is a prediction\n",
    "        return neoml.AutoDiff.max(0., 1. - data * labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-rebecca",
   "metadata": {},
   "source": [
    "Now we'll create the custom loss layer using `neoml.Dnn.CustomLoss(...)`, with a `HingeLossCalculator()` instance as `loss_calculator` parameter, and then connect this layer, and the correct labels source layer it requires, to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional source for class labels\n",
    "label = neoml.Dnn.Source(dnn, name='label')\n",
    "# Custom loss layer with HingeLossCalculator\n",
    "loss = neoml.Dnn.CustomLoss((fc2, label), name='loss',\n",
    "                             loss_calculator=HingeLossCalculator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-cattle",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-jenny",
   "metadata": {},
   "source": [
    "Hinge loss solves a binary classification task with `+/-1` class labels.\n",
    "\n",
    "Let's generate some random data for classification and train the network on it.\n",
    "\n",
    "The data is generated in the following way:\n",
    "\n",
    "- each object is a vector of `128` elements\n",
    "- `+1` class objects are vectors with elements from `N(0.25, 1)` distribution\n",
    "- `-1` class objects are vectors with elements from `N(-0.25, 1)` distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stopped-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss values (per iter)\n",
      "1.3969335556030273\n",
      "1.175158977508545\n",
      "0.37655696272850037\n",
      "0.37330472469329834\n",
      "0.15725426375865936\n",
      "0.10323527455329895\n",
      "0.044364433735609055\n",
      "0.010994041338562965\n",
      "0.0\n",
      "0.021620113402605057\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "channels = 128\n",
    "\n",
    "print('Loss values (per iter)')\n",
    "for _ in range(10):\n",
    "    data_shape = (1, batch_size, 1, 1, 1, 1, channels)\n",
    "    # Each class gets half of the batch\n",
    "    data_ndarr = np.vstack((np.random.normal(0.25, 1., (batch_size // 2, channels)),\n",
    "                            np.random.normal(-0.25, 1., (batch_size // 2, channels))))\n",
    "    data_blob = neoml.Blob.asblob(math_engine, data_ndarr.astype(np.float32), data_shape)\n",
    "    label_shape = (1, batch_size, 1, 1, 1, 1, 1)\n",
    "    # Each class gets half of the batch\n",
    "    label_ndarr = np.vstack((np.ones(batch_size//2),\n",
    "                             -np.ones(batch_size//2)))\n",
    "    label_blob = neoml.Blob.asblob(math_engine, label_ndarr.astype(np.float32), label_shape)\n",
    "    # Train the network on the generated data\n",
    "    dnn.learn({'data': data_blob, 'label': label_blob})\n",
    "    print(loss.last_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
