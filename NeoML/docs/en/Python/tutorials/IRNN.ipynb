{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "different-thinking",
   "metadata": {},
   "source": [
    "Copyright Â© 2017-2021 ABBYY Production LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustainable-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-alert",
   "metadata": {},
   "source": [
    "# MNIST IRNN tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-vector",
   "metadata": {},
   "source": [
    "This tutorial partially reproduce experiment from the [Identity RNN article](https://arxiv.org/pdf/1504.00941.pdf).\n",
    "\n",
    "It demonstrates that IRNN is especially good when working with long sequences.\n",
    "\n",
    "It transposes MNIST images (28 x 28) into sequences of length 784.\n",
    "\n",
    "The article claims that IRNN can achieve 0.9+ accuracy in these conditions.\n",
    "\n",
    "This tutorial contains the following steps:\n",
    "\n",
    "* Download and prepare MNIST dataset\n",
    "* Build the net\n",
    "* Train the net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-essex",
   "metadata": {},
   "source": [
    "## Download and prepare MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-magnet",
   "metadata": {},
   "source": [
    "First of all, we need to download dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aggregate-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-convergence",
   "metadata": {},
   "source": [
    "Now we need to normalize it and convert to 32-bit datatypes for NeoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize\n",
    "X = (255 - X) * 2 / 255 - 1\n",
    "\n",
    "# Fix data types\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-overall",
   "metadata": {},
   "source": [
    "Finally, we have to split it into train and test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confused-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "train_size = 60000\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-affiliate",
   "metadata": {},
   "source": [
    "## Build the net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-latex",
   "metadata": {},
   "source": [
    "### Create math engine (choose device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-forwarding",
   "metadata": {},
   "source": [
    "First of all we need to create math engine. It's an entity responsible for computational operations and data allocation for neural networks. It determines the device used for neural network training and inference.\n",
    "\n",
    "In this tutorial we'll use single-threaded CPU math engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mature-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neoml\n",
    "\n",
    "math_engine = neoml.MathEngine.CpuMathEngine(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-bouquet",
   "metadata": {},
   "source": [
    "### Build the DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-mambo",
   "metadata": {},
   "source": [
    "First of all we need the `neoml.Dnn.Dnn` object which represents a neural network (a graph of layers). Every net needs a math engine to perform its operations and its math engine can't be changed after creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worthy-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = neoml.Dnn.Dnn(math_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-participation",
   "metadata": {},
   "source": [
    "The data is feeded to the network via special `neoml.Dnn.Source` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forty-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = neoml.Dnn.Source(dnn, 'data')  # Source for data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-bloom",
   "metadata": {},
   "source": [
    "Then we need to transpose data into sequences of 784. We can do that by special `neoml.Dnn.Transpose` layer, which swaps 2 dimensions of the blob.\n",
    "\n",
    "Original data will be wrapped into 2 dimensional blob where `BatchWidth` will be equal to batch size, and `Channesl` will be equal to image size. This layer will transform it into sequences (`BatchLength`) of image size, where each element of the sequence will be of size `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "square-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose = neoml.Dnn.Transpose(data, first_dim='batch_length',\n",
    "                                second_dim='channels', name='transpose')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-chinese",
   "metadata": {},
   "source": [
    "Now we may add `neoml.Dnn.Irnn` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "whole-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "irnn = neoml.Dnn.Irnn(transpose, hidden_size, identity_scale=1.,\n",
    "                      input_weight_std=1e-3, name='irnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-bubble",
   "metadata": {},
   "source": [
    "But recurrent layers in NeoML usually return whole sequences. For experiment reproduction we must take only last elements of them. For this purpose we're gonna use `neoml.Dnn.SubSequence` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "large-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "subseq = neoml.Dnn.SubSequence(irnn, start_pos=-1,\n",
    "                               length=1, name='subseq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-joint",
   "metadata": {},
   "source": [
    "Now we use fully-connected layer to form logits (non-normalized distribution) over MNIST classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adaptive-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "fc = neoml.Dnn.FullyConnected(subseq, n_classes, name='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-harbor",
   "metadata": {},
   "source": [
    "Here we must add additional `neoml.Dnn.Source` for labels and loss layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accurate-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = neoml.Dnn.Source(dnn, 'labels')  # Source for labels\n",
    "loss = neoml.Dnn.CrossEntropyLoss((fc, labels), name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-majority",
   "metadata": {},
   "source": [
    "We'll be calculating accuracy by the means of NeoML. To do this we'll need special `neoml.Dnn.Accuracy` layer (and `neoml.Dnn.Sink` layer for extracting accuracy's output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wicked-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilary layers in order to get statistics\n",
    "accuracy = neoml.Dnn.Accuracy((fc, labels), name='accuracy')\n",
    "# accuracy layers writes its result to its output\n",
    "# We need additional sink layer to extract it\n",
    "accuracy_sink = neoml.Dnn.Sink(accuracy, name='accuracy_sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-january",
   "metadata": {},
   "source": [
    "### Create solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-summer",
   "metadata": {},
   "source": [
    "Solver is an object which is responsible for weight optimization (based on gradient values). In this sample we'll use `neoml.Dnn.AdaptiveGradient` solver (neoml's realization of [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vocational-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-6\n",
    "\n",
    "# Create solver\n",
    "dnn.solver = neoml.Dnn.AdaptiveGradient(math_engine, learning_rate=lr,\n",
    "                                           l1=0., l2=0.,  # No regularization\n",
    "                                           max_gradient_norm=1.,  # clip grad\n",
    "                                           moment_decay_rate=0.9,\n",
    "                                           second_moment_decay_rate=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-fields",
   "metadata": {},
   "source": [
    "## Train the net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-modification",
   "metadata": {},
   "source": [
    "The neoml's nets accept data only as `neoml.Blob.Blob`.\n",
    "\n",
    "Blobs are 7-dimensional arrays located in device memory. Each dimension has a specific purpose:\n",
    "\n",
    "1. `BatchLength` - temporal axis (used in recurrent layers)\n",
    "2. `BatchWidth` - classic batch\n",
    "3. `ListSize` - list axis, used when objects are related to the same entity, but without ordering (unlike `BatchLength`)\n",
    "4. `Height` - height of the image\n",
    "5. `Width` - width of the image\n",
    "6. `Depth` - depth of the 3-dimensional image\n",
    "7. `Channels` - channels of the image (also used when object is a 1-dimensional vector)\n",
    "\n",
    "In our case we will use `ndarray` to split data into batches. Blobs will be created based on these batches right before feeding them to the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "commercial-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irnn_data_iterator(X, y, batch_size, math_engine):\n",
    "    \"\"\"Slices numpy arrays into batches and wraps them in blobs\"\"\"\n",
    "    def make_blob(data, math_engine):\n",
    "        \"\"\"Wraps numpy data into neoml blob\"\"\"\n",
    "        shape = data.shape\n",
    "        if len(shape) == 2:  # data\n",
    "            # Wrap 2-D array into blob of (BatchWidth, Channels) shape\n",
    "            return neoml.Blob.asblob(math_engine, data,\n",
    "                                     (1, shape[0], 1, 1, 1, 1, shape[1]))\n",
    "        elif len(shape) == 1:  # dense labels\n",
    "            # Wrap 1-D array into blob of (BatchWidth,) shape\n",
    "            return neoml.Blob.asblob(math_engine, data,\n",
    "                                     (1, shape[0], 1, 1, 1, 1, 1))\n",
    "        else:\n",
    "            assert(False)\n",
    "\n",
    "    start = 0\n",
    "    data_size = y.shape[0]\n",
    "    while start < data_size:\n",
    "        yield (make_blob(X[start : start+batch_size], math_engine),\n",
    "               make_blob(y[start : start+batch_size], math_engine))\n",
    "        start += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-tomorrow",
   "metadata": {},
   "source": [
    "In order to train net you should call `dnn.learn` with data as its argument.\n",
    "\n",
    "In order to run net without traning you should call `dnn.run` with data as its argument.\n",
    "\n",
    "The data argument is a `dict` where keys are `neoml.Dnn.Source` layers' names and values are corresponding `neoml.Blob.Blob`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "brazilian-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net(X, y, batch_size, dnn, is_train):\n",
    "    \"\"\"Runs dnn on given data\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0.\n",
    "    run_iter = dnn.learn if is_train else dnn.run\n",
    "    math_engine = dnn.math_engine\n",
    "    layers = dnn.layers\n",
    "    loss = layers['loss']\n",
    "    accuracy = layers['accuracy']\n",
    "    sink = layers['accuracy_sink']\n",
    "\n",
    "    accuracy.reset = True  # Reset previous statistics\n",
    "    # Iterate over batches\n",
    "    for X_batch, y_batch in irnn_data_iterator(X, y, batch_size, math_engine):\n",
    "        # Run the network on the batch data\n",
    "        run_iter({'data': X_batch, 'labels': y_batch})\n",
    "        total_loss += loss.last_loss * y_batch.batch_width  # Update epoch loss\n",
    "        accuracy.reset = False  # Don't reset statistics within one epoch\n",
    "\n",
    "    avg_loss = total_loss / y.shape[0]\n",
    "    avg_acc = sink.get_blob().asarray()[0]\n",
    "    run_time = time.time() - start\n",
    "    return avg_loss, avg_acc, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-mexican",
   "metadata": {},
   "source": [
    "Now we can train the net.\n",
    "\n",
    "*Note*: It will take 3-4 hours to train. You may uncomment print statements to see the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acute-affiliation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test acc: 0.9050\n",
      "Wall time: 3h 54min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "batch_size = 40\n",
    "n_epoch = 200\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # Train\n",
    "    train_loss, train_acc, run_time = run_net(X_train, y_train, batch_size,\n",
    "                                      dnn, is_train=True)\n",
    "    # print(f'Train #{epoch}\\tLoss: {train_loss:.4f}\\t'\n",
    "    #       f'Accuracy: {train_acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "    # Test\n",
    "    test_loss, test_acc, run_time = run_net(X_test, y_test, batch_size,\n",
    "                                      dnn, is_train=False)\n",
    "    # print(f'Test  #{epoch}\\tLoss: {test_loss:.4f}\\t'\n",
    "    #       f'Accuracy: {test_acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "print(f'Final test acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-surveillance",
   "metadata": {},
   "source": [
    "As we can see this model has achieved 90+% of accuracy on these long sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
