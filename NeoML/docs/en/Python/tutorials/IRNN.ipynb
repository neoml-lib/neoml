{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "different-thinking",
   "metadata": {},
   "source": [
    "Copyright Â© 2017-2021 ABBYY Production LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustainable-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-alert",
   "metadata": {},
   "source": [
    "# MNIST IRNN tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-vector",
   "metadata": {},
   "source": [
    "This tutorial partially reproduce experiment from the [Identity RNN article](https://arxiv.org/pdf/1504.00941.pdf).\n",
    "\n",
    "It demonstrates that IRNN is especially good when working with long sequences.\n",
    "\n",
    "It transposes MNIST images (28 x 28) into sequences of length 784.\n",
    "\n",
    "The article claims that IRNN can achieve 0.9+ accuracy in these conditions.\n",
    "\n",
    "This tutorial contains the following steps:\n",
    "\n",
    "* Download and prepare MNIST dataset\n",
    "* Build the net\n",
    "* Train the net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-essex",
   "metadata": {},
   "source": [
    "## Download and prepare MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-magnet",
   "metadata": {},
   "source": [
    "First of all, we need to download dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aggregate-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-convergence",
   "metadata": {},
   "source": [
    "Now we need to normalize it and convert to 32-bit datatypes for NeoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize\n",
    "X = (255 - X) * 2 / 255 - 1\n",
    "\n",
    "# Fix data types\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-overall",
   "metadata": {},
   "source": [
    "Finally, we have to split it into train and test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confused-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "train_size = 60000\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-affiliate",
   "metadata": {},
   "source": [
    "## Build the net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-latex",
   "metadata": {},
   "source": [
    "### Create math engine (choose device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-forwarding",
   "metadata": {},
   "source": [
    "First of all we need to create math engine. It's an entity responsible for computational operations and data allocation for neural networks. It determines the device used for neural network training and inference.\n",
    "\n",
    "In this tutorial we'll use single-threaded CPU math engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mature-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neoml\n",
    "\n",
    "math_engine = neoml.MathEngine.CpuMathEngine(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-bouquet",
   "metadata": {},
   "source": [
    "### Build the DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-mambo",
   "metadata": {},
   "source": [
    "First of all we need the `neoml.Dnn.Dnn` object which represents a neural network (a graph of layers). Every net needs a math engine to perform its operations and its math engine can't be changed after creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worthy-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = neoml.Dnn.Dnn(math_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-participation",
   "metadata": {},
   "source": [
    "The data is feeded to the network via special `neoml.Dnn.Source` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forty-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = neoml.Dnn.Source(dnn, 'data')  # Source for data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-bloom",
   "metadata": {},
   "source": [
    "Then we need to transpose data into sequences of 784. We can do that by special `neoml.Dnn.Transpose` layer, which swaps 2 dimensions of the blob.\n",
    "\n",
    "Original data will be wrapped into 2 dimensional blob where `BatchWidth` will be equal to batch size, and `Channesl` will be equal to image size. This layer will transform it into sequences (`BatchLength`) of image size, where each element of the sequence will be of size `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "square-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose = neoml.Dnn.Transpose(data, first_dim='batch_length',\n",
    "                                second_dim='channels', name='transpose')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-chinese",
   "metadata": {},
   "source": [
    "Now we may add `neoml.Dnn.Irnn` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "whole-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "irnn = neoml.Dnn.Irnn(transpose, hidden_size, identity_scale=1.,\n",
    "                      input_weight_std=1e-3, name='irnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-bubble",
   "metadata": {},
   "source": [
    "But recurrent layers in NeoML usually return whole sequences. For experiment reproduction we must take only last elements of them. For this purpose we're gonna use `neoml.Dnn.SubSequence` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "large-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "subseq = neoml.Dnn.SubSequence(irnn, start_pos=-1,\n",
    "                               length=1, name='subseq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-joint",
   "metadata": {},
   "source": [
    "Now we use fully-connected layer to form logits (non-normalized distribution) over MNIST classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adaptive-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "fc = neoml.Dnn.FullyConnected(subseq, n_classes, name='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-harbor",
   "metadata": {},
   "source": [
    "Here we must add additional `neoml.Dnn.Source` for labels and loss layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accurate-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = neoml.Dnn.Source(dnn, 'labels')  # Source for labels\n",
    "loss = neoml.Dnn.CrossEntropyLoss((fc, labels), name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-majority",
   "metadata": {},
   "source": [
    "We'll be calculating accuracy by the means of NeoML. To do this we'll need special `neoml.Dnn.Accuracy` layer (and `neoml.Dnn.Sink` layer for extracting accuracy's output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wicked-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilary layers in order to get statistics\n",
    "accuracy = neoml.Dnn.Accuracy((fc, labels), name='accuracy')\n",
    "# accuracy layers writes its result to its output\n",
    "# We need additional sink layer to extract it\n",
    "accuracy_sink = neoml.Dnn.Sink(accuracy, name='accuracy_sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-january",
   "metadata": {},
   "source": [
    "### Create solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-summer",
   "metadata": {},
   "source": [
    "Solver is an object which is responsible for weight optimization (based on gradient values). In this sample we'll use `neoml.Dnn.AdaptiveGradient` solver (neoml's realization of [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vocational-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-6\n",
    "\n",
    "# Create solver\n",
    "dnn.solver = neoml.Dnn.AdaptiveGradient(math_engine, learning_rate=lr,\n",
    "                                           l1=0., l2=0.,  # No regularization\n",
    "                                           max_gradient_norm=1.,  # clip grad\n",
    "                                           moment_decay_rate=0.9,\n",
    "                                           second_moment_decay_rate=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-fields",
   "metadata": {},
   "source": [
    "## Train the net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-modification",
   "metadata": {},
   "source": [
    "The neoml's nets accept data only as `neoml.Blob.Blob`.\n",
    "\n",
    "Blobs are 7-dimensional arrays located in device memory. Each dimension has a specific purpose:\n",
    "\n",
    "1. `BatchLength` - temporal axis (used in recurrent layers)\n",
    "2. `BatchWidth` - classic batch\n",
    "3. `ListSize` - list axis, used when objects are related to the same entity, but without ordering (unlike `BatchLength`)\n",
    "4. `Height` - height of the image\n",
    "5. `Width` - width of the image\n",
    "6. `Depth` - depth of the 3-dimensional image\n",
    "7. `Channels` - channels of the image (also used when object is a 1-dimensional vector)\n",
    "\n",
    "In our case we will use `ndarray` to split data into batches. Blobs will be created based on these batches right before feeding them to the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "commercial-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irnn_data_iterator(X, y, batch_size, math_engine):\n",
    "    \"\"\"Slices numpy arrays into batches and wraps them in blobs\"\"\"\n",
    "    def make_blob(data, math_engine):\n",
    "        \"\"\"Wraps numpy data into neoml blob\"\"\"\n",
    "        shape = data.shape\n",
    "        if len(shape) == 2:  # data\n",
    "            # Wrap 2-D array into blob of (BatchWidth, Channels) shape\n",
    "            return neoml.Blob.asblob(math_engine, data,\n",
    "                                     (1, shape[0], 1, 1, 1, 1, shape[1]))\n",
    "        elif len(shape) == 1:  # dense labels\n",
    "            # Wrap 1-D array into blob of (BatchWidth,) shape\n",
    "            return neoml.Blob.asblob(math_engine, data,\n",
    "                                     (1, shape[0], 1, 1, 1, 1, 1))\n",
    "        else:\n",
    "            assert(False)\n",
    "\n",
    "    start = 0\n",
    "    data_size = y.shape[0]\n",
    "    while start < data_size:\n",
    "        yield (make_blob(X[start : start+batch_size], math_engine),\n",
    "               make_blob(y[start : start+batch_size], math_engine))\n",
    "        start += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-tomorrow",
   "metadata": {},
   "source": [
    "In order to train net you should call `dnn.learn` with data as its argument.\n",
    "\n",
    "In order to run net without traning you should call `dnn.run` with data as its argument.\n",
    "\n",
    "The data argument is a `dict` where keys are `neoml.Dnn.Source` layers' names and values are corresponding `neoml.Blob.Blob`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "brazilian-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net(X, y, batch_size, dnn, is_train):\n",
    "    \"\"\"Runs dnn on given data\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0.\n",
    "    run_iter = dnn.learn if is_train else dnn.run\n",
    "    math_engine = dnn.math_engine\n",
    "    layers = dnn.layers\n",
    "    loss = layers['loss']\n",
    "    accuracy = layers['accuracy']\n",
    "    sink = layers['accuracy_sink']\n",
    "\n",
    "    accuracy.reset = True  # Reset previous statistics\n",
    "    # Iterate over batches\n",
    "    for X_batch, y_batch in irnn_data_iterator(X, y, batch_size, math_engine):\n",
    "        # Run the network on the batch data\n",
    "        run_iter({'data': X_batch, 'labels': y_batch})\n",
    "        total_loss += loss.last_loss * y_batch.batch_width  # Update epoch loss\n",
    "        accuracy.reset = False  # Don't reset statistics within one epoch\n",
    "\n",
    "    avg_loss = total_loss / y.shape[0]\n",
    "    avg_acc = sink.get_blob().asarray()[0]\n",
    "    run_time = time.time() - start\n",
    "    return avg_loss, avg_acc, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-mexican",
   "metadata": {},
   "source": [
    "Now we can train the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acute-affiliation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train #0\tLoss: 1.9263\tAccuracy: 0.3174\tTime: 69.16 sec\n",
      "Test  #0\tLoss: 1.7162\tAccuracy: 0.4156\tTime: 3.80 sec\n",
      "Train #1\tLoss: 1.4877\tAccuracy: 0.4865\tTime: 69.03 sec\n",
      "Test  #1\tLoss: 1.3827\tAccuracy: 0.5215\tTime: 3.77 sec\n",
      "Train #2\tLoss: 1.3253\tAccuracy: 0.5350\tTime: 69.18 sec\n",
      "Test  #2\tLoss: 1.2880\tAccuracy: 0.5417\tTime: 3.79 sec\n",
      "Train #3\tLoss: 1.2597\tAccuracy: 0.5592\tTime: 68.88 sec\n",
      "Test  #3\tLoss: 1.2394\tAccuracy: 0.5564\tTime: 3.77 sec\n",
      "Train #4\tLoss: 1.2006\tAccuracy: 0.5789\tTime: 69.04 sec\n",
      "Test  #4\tLoss: 1.1496\tAccuracy: 0.5932\tTime: 3.78 sec\n",
      "Train #5\tLoss: 1.1440\tAccuracy: 0.5961\tTime: 68.92 sec\n",
      "Test  #5\tLoss: 1.1104\tAccuracy: 0.6021\tTime: 3.78 sec\n",
      "Train #6\tLoss: 1.0968\tAccuracy: 0.6109\tTime: 68.91 sec\n",
      "Test  #6\tLoss: 1.0676\tAccuracy: 0.6231\tTime: 3.79 sec\n",
      "Train #7\tLoss: 1.0419\tAccuracy: 0.6324\tTime: 68.90 sec\n",
      "Test  #7\tLoss: 1.0214\tAccuracy: 0.6388\tTime: 3.78 sec\n",
      "Train #8\tLoss: 1.0067\tAccuracy: 0.6418\tTime: 68.94 sec\n",
      "Test  #8\tLoss: 0.9871\tAccuracy: 0.6538\tTime: 3.77 sec\n",
      "Train #9\tLoss: 0.9841\tAccuracy: 0.6498\tTime: 68.97 sec\n",
      "Test  #9\tLoss: 0.9720\tAccuracy: 0.6559\tTime: 3.78 sec\n",
      "Train #10\tLoss: 0.9656\tAccuracy: 0.6565\tTime: 69.11 sec\n",
      "Test  #10\tLoss: 0.9700\tAccuracy: 0.6527\tTime: 3.77 sec\n",
      "Train #11\tLoss: 0.9472\tAccuracy: 0.6637\tTime: 68.93 sec\n",
      "Test  #11\tLoss: 0.9946\tAccuracy: 0.6482\tTime: 3.79 sec\n",
      "Train #12\tLoss: 0.9313\tAccuracy: 0.6709\tTime: 68.97 sec\n",
      "Test  #12\tLoss: 1.0068\tAccuracy: 0.6463\tTime: 3.77 sec\n",
      "Train #13\tLoss: 0.9204\tAccuracy: 0.6754\tTime: 68.91 sec\n",
      "Test  #13\tLoss: 1.0162\tAccuracy: 0.6427\tTime: 3.78 sec\n",
      "Train #14\tLoss: 0.9061\tAccuracy: 0.6802\tTime: 69.02 sec\n",
      "Test  #14\tLoss: 1.0367\tAccuracy: 0.6365\tTime: 3.77 sec\n",
      "Train #15\tLoss: 0.8935\tAccuracy: 0.6852\tTime: 68.92 sec\n",
      "Test  #15\tLoss: 1.0144\tAccuracy: 0.6470\tTime: 3.78 sec\n",
      "Train #16\tLoss: 0.8817\tAccuracy: 0.6899\tTime: 69.01 sec\n",
      "Test  #16\tLoss: 0.9843\tAccuracy: 0.6555\tTime: 3.77 sec\n",
      "Train #17\tLoss: 0.8697\tAccuracy: 0.6940\tTime: 68.81 sec\n",
      "Test  #17\tLoss: 0.9368\tAccuracy: 0.6749\tTime: 3.76 sec\n",
      "Train #18\tLoss: 0.8583\tAccuracy: 0.6987\tTime: 68.90 sec\n",
      "Test  #18\tLoss: 0.9159\tAccuracy: 0.6803\tTime: 3.77 sec\n",
      "Train #19\tLoss: 0.8453\tAccuracy: 0.7029\tTime: 68.90 sec\n",
      "Test  #19\tLoss: 0.8849\tAccuracy: 0.6912\tTime: 3.77 sec\n",
      "Train #20\tLoss: 0.8297\tAccuracy: 0.7096\tTime: 68.88 sec\n",
      "Test  #20\tLoss: 0.8949\tAccuracy: 0.6847\tTime: 3.77 sec\n",
      "Train #21\tLoss: 0.8146\tAccuracy: 0.7166\tTime: 68.93 sec\n",
      "Test  #21\tLoss: 0.8686\tAccuracy: 0.6964\tTime: 3.77 sec\n",
      "Train #22\tLoss: 0.7999\tAccuracy: 0.7226\tTime: 68.97 sec\n",
      "Test  #22\tLoss: 0.8437\tAccuracy: 0.7073\tTime: 3.77 sec\n",
      "Train #23\tLoss: 0.7854\tAccuracy: 0.7285\tTime: 69.00 sec\n",
      "Test  #23\tLoss: 0.8221\tAccuracy: 0.7142\tTime: 3.79 sec\n",
      "Train #24\tLoss: 0.7709\tAccuracy: 0.7340\tTime: 69.22 sec\n",
      "Test  #24\tLoss: 0.8110\tAccuracy: 0.7190\tTime: 3.77 sec\n",
      "Train #25\tLoss: 0.7566\tAccuracy: 0.7395\tTime: 69.00 sec\n",
      "Test  #25\tLoss: 0.8006\tAccuracy: 0.7246\tTime: 3.77 sec\n",
      "Train #26\tLoss: 0.7409\tAccuracy: 0.7461\tTime: 68.88 sec\n",
      "Test  #26\tLoss: 0.7757\tAccuracy: 0.7374\tTime: 3.79 sec\n",
      "Train #27\tLoss: 0.7266\tAccuracy: 0.7520\tTime: 68.89 sec\n",
      "Test  #27\tLoss: 0.7591\tAccuracy: 0.7470\tTime: 3.79 sec\n",
      "Train #28\tLoss: 0.7106\tAccuracy: 0.7573\tTime: 69.48 sec\n",
      "Test  #28\tLoss: 0.7450\tAccuracy: 0.7523\tTime: 3.80 sec\n",
      "Train #29\tLoss: 0.6945\tAccuracy: 0.7621\tTime: 68.99 sec\n",
      "Test  #29\tLoss: 0.7352\tAccuracy: 0.7566\tTime: 3.77 sec\n",
      "Train #30\tLoss: 0.6778\tAccuracy: 0.7691\tTime: 69.03 sec\n",
      "Test  #30\tLoss: 0.7154\tAccuracy: 0.7614\tTime: 3.78 sec\n",
      "Train #31\tLoss: 0.6618\tAccuracy: 0.7760\tTime: 69.00 sec\n",
      "Test  #31\tLoss: 0.6930\tAccuracy: 0.7672\tTime: 3.78 sec\n",
      "Train #32\tLoss: 0.6470\tAccuracy: 0.7805\tTime: 69.14 sec\n",
      "Test  #32\tLoss: 0.6745\tAccuracy: 0.7732\tTime: 3.78 sec\n",
      "Train #33\tLoss: 0.6322\tAccuracy: 0.7861\tTime: 68.99 sec\n",
      "Test  #33\tLoss: 0.6488\tAccuracy: 0.7818\tTime: 3.78 sec\n",
      "Train #34\tLoss: 0.6172\tAccuracy: 0.7907\tTime: 68.98 sec\n",
      "Test  #34\tLoss: 0.6292\tAccuracy: 0.7877\tTime: 3.78 sec\n",
      "Train #35\tLoss: 0.6055\tAccuracy: 0.7944\tTime: 68.94 sec\n",
      "Test  #35\tLoss: 0.6180\tAccuracy: 0.7919\tTime: 3.78 sec\n",
      "Train #36\tLoss: 0.5944\tAccuracy: 0.7987\tTime: 69.03 sec\n",
      "Test  #36\tLoss: 0.6137\tAccuracy: 0.7945\tTime: 3.77 sec\n",
      "Train #37\tLoss: 0.5847\tAccuracy: 0.8019\tTime: 69.00 sec\n",
      "Test  #37\tLoss: 0.6104\tAccuracy: 0.7975\tTime: 3.78 sec\n",
      "Train #38\tLoss: 0.5757\tAccuracy: 0.8044\tTime: 69.24 sec\n",
      "Test  #38\tLoss: 0.6056\tAccuracy: 0.7983\tTime: 3.78 sec\n",
      "Train #39\tLoss: 0.5671\tAccuracy: 0.8076\tTime: 68.98 sec\n",
      "Test  #39\tLoss: 0.6086\tAccuracy: 0.7992\tTime: 3.78 sec\n",
      "Train #40\tLoss: 0.5591\tAccuracy: 0.8100\tTime: 68.99 sec\n",
      "Test  #40\tLoss: 0.6178\tAccuracy: 0.7964\tTime: 3.78 sec\n",
      "Train #41\tLoss: 0.5511\tAccuracy: 0.8134\tTime: 68.99 sec\n",
      "Test  #41\tLoss: 0.6148\tAccuracy: 0.7973\tTime: 3.78 sec\n",
      "Train #42\tLoss: 0.5435\tAccuracy: 0.8158\tTime: 69.02 sec\n",
      "Test  #42\tLoss: 0.6346\tAccuracy: 0.7910\tTime: 3.79 sec\n",
      "Train #43\tLoss: 0.5361\tAccuracy: 0.8187\tTime: 69.07 sec\n",
      "Test  #43\tLoss: 0.6116\tAccuracy: 0.7979\tTime: 3.79 sec\n",
      "Train #44\tLoss: 0.5293\tAccuracy: 0.8208\tTime: 69.00 sec\n",
      "Test  #44\tLoss: 0.6158\tAccuracy: 0.7963\tTime: 3.78 sec\n",
      "Train #45\tLoss: 0.5227\tAccuracy: 0.8237\tTime: 69.00 sec\n",
      "Test  #45\tLoss: 0.6247\tAccuracy: 0.7919\tTime: 3.78 sec\n",
      "Train #46\tLoss: 0.5168\tAccuracy: 0.8250\tTime: 69.02 sec\n",
      "Test  #46\tLoss: 0.6219\tAccuracy: 0.7947\tTime: 3.77 sec\n",
      "Train #47\tLoss: 0.5114\tAccuracy: 0.8271\tTime: 69.06 sec\n",
      "Test  #47\tLoss: 0.6175\tAccuracy: 0.7976\tTime: 3.78 sec\n",
      "Train #48\tLoss: 0.5053\tAccuracy: 0.8295\tTime: 69.13 sec\n",
      "Test  #48\tLoss: 0.6117\tAccuracy: 0.7984\tTime: 3.82 sec\n",
      "Train #49\tLoss: 0.4999\tAccuracy: 0.8306\tTime: 70.00 sec\n",
      "Test  #49\tLoss: 0.5995\tAccuracy: 0.8020\tTime: 3.81 sec\n",
      "Train #50\tLoss: 0.4955\tAccuracy: 0.8315\tTime: 69.21 sec\n",
      "Test  #50\tLoss: 0.5900\tAccuracy: 0.8051\tTime: 3.77 sec\n",
      "Train #51\tLoss: 0.4910\tAccuracy: 0.8330\tTime: 69.27 sec\n",
      "Test  #51\tLoss: 0.5818\tAccuracy: 0.8073\tTime: 3.78 sec\n",
      "Train #52\tLoss: 0.4864\tAccuracy: 0.8340\tTime: 69.04 sec\n",
      "Test  #52\tLoss: 0.5712\tAccuracy: 0.8112\tTime: 3.78 sec\n",
      "Train #53\tLoss: 0.4815\tAccuracy: 0.8347\tTime: 69.55 sec\n",
      "Test  #53\tLoss: 0.5638\tAccuracy: 0.8143\tTime: 3.79 sec\n",
      "Train #54\tLoss: 0.4766\tAccuracy: 0.8368\tTime: 69.13 sec\n",
      "Test  #54\tLoss: 0.5576\tAccuracy: 0.8165\tTime: 3.78 sec\n",
      "Train #55\tLoss: 0.4728\tAccuracy: 0.8382\tTime: 69.06 sec\n",
      "Test  #55\tLoss: 0.5469\tAccuracy: 0.8203\tTime: 3.79 sec\n",
      "Train #56\tLoss: 0.4691\tAccuracy: 0.8391\tTime: 68.96 sec\n",
      "Test  #56\tLoss: 0.5444\tAccuracy: 0.8218\tTime: 3.80 sec\n",
      "Train #57\tLoss: 0.4652\tAccuracy: 0.8400\tTime: 68.98 sec\n",
      "Test  #57\tLoss: 0.5381\tAccuracy: 0.8243\tTime: 3.78 sec\n",
      "Train #58\tLoss: 0.4610\tAccuracy: 0.8413\tTime: 69.09 sec\n",
      "Test  #58\tLoss: 0.5293\tAccuracy: 0.8265\tTime: 3.79 sec\n",
      "Train #59\tLoss: 0.4568\tAccuracy: 0.8429\tTime: 69.02 sec\n",
      "Test  #59\tLoss: 0.5233\tAccuracy: 0.8286\tTime: 3.79 sec\n",
      "Train #60\tLoss: 0.4526\tAccuracy: 0.8444\tTime: 68.96 sec\n",
      "Test  #60\tLoss: 0.5184\tAccuracy: 0.8300\tTime: 3.77 sec\n",
      "Train #61\tLoss: 0.4483\tAccuracy: 0.8462\tTime: 69.02 sec\n",
      "Test  #61\tLoss: 0.5043\tAccuracy: 0.8335\tTime: 3.77 sec\n",
      "Train #62\tLoss: 0.4437\tAccuracy: 0.8476\tTime: 69.03 sec\n",
      "Test  #62\tLoss: 0.5021\tAccuracy: 0.8343\tTime: 3.77 sec\n",
      "Train #63\tLoss: 0.4398\tAccuracy: 0.8494\tTime: 69.03 sec\n",
      "Test  #63\tLoss: 0.4936\tAccuracy: 0.8363\tTime: 3.81 sec\n",
      "Train #64\tLoss: 0.4352\tAccuracy: 0.8507\tTime: 69.06 sec\n",
      "Test  #64\tLoss: 0.4879\tAccuracy: 0.8391\tTime: 3.78 sec\n",
      "Train #65\tLoss: 0.4308\tAccuracy: 0.8516\tTime: 69.19 sec\n",
      "Test  #65\tLoss: 0.4800\tAccuracy: 0.8406\tTime: 3.80 sec\n",
      "Train #66\tLoss: 0.4271\tAccuracy: 0.8528\tTime: 69.08 sec\n",
      "Test  #66\tLoss: 0.4776\tAccuracy: 0.8418\tTime: 3.79 sec\n",
      "Train #67\tLoss: 0.4228\tAccuracy: 0.8544\tTime: 69.05 sec\n",
      "Test  #67\tLoss: 0.4717\tAccuracy: 0.8424\tTime: 3.77 sec\n",
      "Train #68\tLoss: 0.4188\tAccuracy: 0.8557\tTime: 69.10 sec\n",
      "Test  #68\tLoss: 0.4625\tAccuracy: 0.8448\tTime: 3.77 sec\n",
      "Train #69\tLoss: 0.4148\tAccuracy: 0.8563\tTime: 68.93 sec\n",
      "Test  #69\tLoss: 0.4596\tAccuracy: 0.8463\tTime: 3.78 sec\n",
      "Train #70\tLoss: 0.4110\tAccuracy: 0.8581\tTime: 69.15 sec\n",
      "Test  #70\tLoss: 0.4566\tAccuracy: 0.8470\tTime: 3.78 sec\n",
      "Train #71\tLoss: 0.4074\tAccuracy: 0.8600\tTime: 69.07 sec\n",
      "Test  #71\tLoss: 0.4512\tAccuracy: 0.8501\tTime: 3.78 sec\n",
      "Train #72\tLoss: 0.4037\tAccuracy: 0.8618\tTime: 69.08 sec\n",
      "Test  #72\tLoss: 0.4486\tAccuracy: 0.8507\tTime: 3.78 sec\n",
      "Train #73\tLoss: 0.4001\tAccuracy: 0.8634\tTime: 69.06 sec\n",
      "Test  #73\tLoss: 0.4448\tAccuracy: 0.8512\tTime: 3.78 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train #74\tLoss: 0.3969\tAccuracy: 0.8649\tTime: 69.16 sec\n",
      "Test  #74\tLoss: 0.4424\tAccuracy: 0.8512\tTime: 3.78 sec\n",
      "Train #75\tLoss: 0.3936\tAccuracy: 0.8658\tTime: 69.09 sec\n",
      "Test  #75\tLoss: 0.4402\tAccuracy: 0.8517\tTime: 3.80 sec\n",
      "Train #76\tLoss: 0.3904\tAccuracy: 0.8670\tTime: 69.06 sec\n",
      "Test  #76\tLoss: 0.4350\tAccuracy: 0.8522\tTime: 3.78 sec\n",
      "Train #77\tLoss: 0.3873\tAccuracy: 0.8681\tTime: 69.02 sec\n",
      "Test  #77\tLoss: 0.4276\tAccuracy: 0.8537\tTime: 3.79 sec\n",
      "Train #78\tLoss: 0.3841\tAccuracy: 0.8695\tTime: 69.04 sec\n",
      "Test  #78\tLoss: 0.4219\tAccuracy: 0.8572\tTime: 3.78 sec\n",
      "Train #79\tLoss: 0.3807\tAccuracy: 0.8708\tTime: 69.14 sec\n",
      "Test  #79\tLoss: 0.4183\tAccuracy: 0.8588\tTime: 3.79 sec\n",
      "Train #80\tLoss: 0.3774\tAccuracy: 0.8728\tTime: 69.07 sec\n",
      "Test  #80\tLoss: 0.4157\tAccuracy: 0.8583\tTime: 3.78 sec\n",
      "Train #81\tLoss: 0.3745\tAccuracy: 0.8739\tTime: 69.10 sec\n",
      "Test  #81\tLoss: 0.4075\tAccuracy: 0.8626\tTime: 3.78 sec\n",
      "Train #82\tLoss: 0.3720\tAccuracy: 0.8745\tTime: 69.00 sec\n",
      "Test  #82\tLoss: 0.4053\tAccuracy: 0.8639\tTime: 3.79 sec\n",
      "Train #83\tLoss: 0.3690\tAccuracy: 0.8760\tTime: 69.07 sec\n",
      "Test  #83\tLoss: 0.3976\tAccuracy: 0.8656\tTime: 3.78 sec\n",
      "Train #84\tLoss: 0.3663\tAccuracy: 0.8769\tTime: 68.96 sec\n",
      "Test  #84\tLoss: 0.3914\tAccuracy: 0.8690\tTime: 3.79 sec\n",
      "Train #85\tLoss: 0.3641\tAccuracy: 0.8780\tTime: 69.11 sec\n",
      "Test  #85\tLoss: 0.3892\tAccuracy: 0.8709\tTime: 3.78 sec\n",
      "Train #86\tLoss: 0.3617\tAccuracy: 0.8790\tTime: 69.00 sec\n",
      "Test  #86\tLoss: 0.3885\tAccuracy: 0.8716\tTime: 3.78 sec\n",
      "Train #87\tLoss: 0.3590\tAccuracy: 0.8800\tTime: 68.97 sec\n",
      "Test  #87\tLoss: 0.3878\tAccuracy: 0.8708\tTime: 3.78 sec\n",
      "Train #88\tLoss: 0.3568\tAccuracy: 0.8805\tTime: 69.00 sec\n",
      "Test  #88\tLoss: 0.3812\tAccuracy: 0.8731\tTime: 3.78 sec\n",
      "Train #89\tLoss: 0.3541\tAccuracy: 0.8817\tTime: 69.02 sec\n",
      "Test  #89\tLoss: 0.3778\tAccuracy: 0.8745\tTime: 3.79 sec\n",
      "Train #90\tLoss: 0.3520\tAccuracy: 0.8823\tTime: 69.04 sec\n",
      "Test  #90\tLoss: 0.3807\tAccuracy: 0.8736\tTime: 3.77 sec\n",
      "Train #91\tLoss: 0.3493\tAccuracy: 0.8830\tTime: 69.05 sec\n",
      "Test  #91\tLoss: 0.3800\tAccuracy: 0.8760\tTime: 3.77 sec\n",
      "Train #92\tLoss: 0.3474\tAccuracy: 0.8838\tTime: 69.26 sec\n",
      "Test  #92\tLoss: 0.3784\tAccuracy: 0.8756\tTime: 3.77 sec\n",
      "Train #93\tLoss: 0.3452\tAccuracy: 0.8847\tTime: 69.02 sec\n",
      "Test  #93\tLoss: 0.3796\tAccuracy: 0.8748\tTime: 3.79 sec\n",
      "Train #94\tLoss: 0.3435\tAccuracy: 0.8852\tTime: 68.99 sec\n",
      "Test  #94\tLoss: 0.3768\tAccuracy: 0.8766\tTime: 3.77 sec\n",
      "Train #95\tLoss: 0.3416\tAccuracy: 0.8857\tTime: 69.00 sec\n",
      "Test  #95\tLoss: 0.3717\tAccuracy: 0.8789\tTime: 3.78 sec\n",
      "Train #96\tLoss: 0.3399\tAccuracy: 0.8865\tTime: 69.06 sec\n",
      "Test  #96\tLoss: 0.3708\tAccuracy: 0.8793\tTime: 3.79 sec\n",
      "Train #97\tLoss: 0.3381\tAccuracy: 0.8867\tTime: 69.04 sec\n",
      "Test  #97\tLoss: 0.3673\tAccuracy: 0.8800\tTime: 3.78 sec\n",
      "Train #98\tLoss: 0.3363\tAccuracy: 0.8878\tTime: 69.07 sec\n",
      "Test  #98\tLoss: 0.3670\tAccuracy: 0.8803\tTime: 3.78 sec\n",
      "Train #99\tLoss: 0.3345\tAccuracy: 0.8885\tTime: 69.02 sec\n",
      "Test  #99\tLoss: 0.3657\tAccuracy: 0.8804\tTime: 3.77 sec\n",
      "Train #100\tLoss: 0.3325\tAccuracy: 0.8895\tTime: 68.95 sec\n",
      "Test  #100\tLoss: 0.3628\tAccuracy: 0.8813\tTime: 3.78 sec\n",
      "Train #101\tLoss: 0.3309\tAccuracy: 0.8900\tTime: 69.10 sec\n",
      "Test  #101\tLoss: 0.3609\tAccuracy: 0.8827\tTime: 3.79 sec\n",
      "Train #102\tLoss: 0.3289\tAccuracy: 0.8905\tTime: 68.99 sec\n",
      "Test  #102\tLoss: 0.3610\tAccuracy: 0.8810\tTime: 3.78 sec\n",
      "Train #103\tLoss: 0.3273\tAccuracy: 0.8914\tTime: 69.00 sec\n",
      "Test  #103\tLoss: 0.3606\tAccuracy: 0.8816\tTime: 3.77 sec\n",
      "Train #104\tLoss: 0.3256\tAccuracy: 0.8922\tTime: 69.04 sec\n",
      "Test  #104\tLoss: 0.3579\tAccuracy: 0.8821\tTime: 3.78 sec\n",
      "Train #105\tLoss: 0.3241\tAccuracy: 0.8923\tTime: 69.09 sec\n",
      "Test  #105\tLoss: 0.3561\tAccuracy: 0.8824\tTime: 3.87 sec\n",
      "Train #106\tLoss: 0.3228\tAccuracy: 0.8929\tTime: 68.99 sec\n",
      "Test  #106\tLoss: 0.3545\tAccuracy: 0.8828\tTime: 3.79 sec\n",
      "Train #107\tLoss: 0.3212\tAccuracy: 0.8938\tTime: 68.96 sec\n",
      "Test  #107\tLoss: 0.3523\tAccuracy: 0.8846\tTime: 3.79 sec\n",
      "Train #108\tLoss: 0.3195\tAccuracy: 0.8938\tTime: 69.04 sec\n",
      "Test  #108\tLoss: 0.3513\tAccuracy: 0.8847\tTime: 3.78 sec\n",
      "Train #109\tLoss: 0.3180\tAccuracy: 0.8944\tTime: 69.00 sec\n",
      "Test  #109\tLoss: 0.3515\tAccuracy: 0.8838\tTime: 3.78 sec\n",
      "Train #110\tLoss: 0.3168\tAccuracy: 0.8947\tTime: 69.05 sec\n",
      "Test  #110\tLoss: 0.3519\tAccuracy: 0.8838\tTime: 3.78 sec\n",
      "Train #111\tLoss: 0.3158\tAccuracy: 0.8949\tTime: 68.98 sec\n",
      "Test  #111\tLoss: 0.3523\tAccuracy: 0.8822\tTime: 3.80 sec\n",
      "Train #112\tLoss: 0.3139\tAccuracy: 0.8954\tTime: 69.00 sec\n",
      "Test  #112\tLoss: 0.3524\tAccuracy: 0.8826\tTime: 3.79 sec\n",
      "Train #113\tLoss: 0.3125\tAccuracy: 0.8960\tTime: 69.02 sec\n",
      "Test  #113\tLoss: 0.3482\tAccuracy: 0.8834\tTime: 3.77 sec\n",
      "Train #114\tLoss: 0.3112\tAccuracy: 0.8965\tTime: 69.32 sec\n",
      "Test  #114\tLoss: 0.3488\tAccuracy: 0.8829\tTime: 3.79 sec\n",
      "Train #115\tLoss: 0.3098\tAccuracy: 0.8964\tTime: 69.08 sec\n",
      "Test  #115\tLoss: 0.3489\tAccuracy: 0.8835\tTime: 3.78 sec\n",
      "Train #116\tLoss: 0.3085\tAccuracy: 0.8969\tTime: 69.03 sec\n",
      "Test  #116\tLoss: 0.3500\tAccuracy: 0.8828\tTime: 3.77 sec\n",
      "Train #117\tLoss: 0.3072\tAccuracy: 0.8974\tTime: 69.03 sec\n",
      "Test  #117\tLoss: 0.3459\tAccuracy: 0.8842\tTime: 3.78 sec\n",
      "Train #118\tLoss: 0.3059\tAccuracy: 0.8979\tTime: 69.25 sec\n",
      "Test  #118\tLoss: 0.3454\tAccuracy: 0.8851\tTime: 3.81 sec\n",
      "Train #119\tLoss: 0.3046\tAccuracy: 0.8985\tTime: 69.02 sec\n",
      "Test  #119\tLoss: 0.3436\tAccuracy: 0.8860\tTime: 3.78 sec\n",
      "Train #120\tLoss: 0.3036\tAccuracy: 0.8987\tTime: 69.01 sec\n",
      "Test  #120\tLoss: 0.3439\tAccuracy: 0.8861\tTime: 3.78 sec\n",
      "Train #121\tLoss: 0.3022\tAccuracy: 0.8989\tTime: 69.04 sec\n",
      "Test  #121\tLoss: 0.3435\tAccuracy: 0.8855\tTime: 3.77 sec\n",
      "Train #122\tLoss: 0.3012\tAccuracy: 0.8989\tTime: 69.16 sec\n",
      "Test  #122\tLoss: 0.3439\tAccuracy: 0.8849\tTime: 3.80 sec\n",
      "Train #123\tLoss: 0.3002\tAccuracy: 0.8995\tTime: 69.14 sec\n",
      "Test  #123\tLoss: 0.3444\tAccuracy: 0.8841\tTime: 3.79 sec\n",
      "Train #124\tLoss: 0.2989\tAccuracy: 0.9001\tTime: 69.22 sec\n",
      "Test  #124\tLoss: 0.3447\tAccuracy: 0.8841\tTime: 3.78 sec\n",
      "Train #125\tLoss: 0.2980\tAccuracy: 0.9000\tTime: 69.09 sec\n",
      "Test  #125\tLoss: 0.3426\tAccuracy: 0.8853\tTime: 3.78 sec\n",
      "Train #126\tLoss: 0.2966\tAccuracy: 0.9009\tTime: 69.11 sec\n",
      "Test  #126\tLoss: 0.3410\tAccuracy: 0.8858\tTime: 3.77 sec\n",
      "Train #127\tLoss: 0.2952\tAccuracy: 0.9007\tTime: 69.03 sec\n",
      "Test  #127\tLoss: 0.3421\tAccuracy: 0.8853\tTime: 3.78 sec\n",
      "Train #128\tLoss: 0.2940\tAccuracy: 0.9010\tTime: 69.01 sec\n",
      "Test  #128\tLoss: 0.3435\tAccuracy: 0.8856\tTime: 3.79 sec\n",
      "Train #129\tLoss: 0.2931\tAccuracy: 0.9015\tTime: 68.92 sec\n",
      "Test  #129\tLoss: 0.3401\tAccuracy: 0.8863\tTime: 3.78 sec\n",
      "Train #130\tLoss: 0.2920\tAccuracy: 0.9020\tTime: 69.04 sec\n",
      "Test  #130\tLoss: 0.3392\tAccuracy: 0.8870\tTime: 3.78 sec\n",
      "Train #131\tLoss: 0.2912\tAccuracy: 0.9021\tTime: 69.05 sec\n",
      "Test  #131\tLoss: 0.3406\tAccuracy: 0.8869\tTime: 3.78 sec\n",
      "Train #132\tLoss: 0.2900\tAccuracy: 0.9029\tTime: 69.19 sec\n",
      "Test  #132\tLoss: 0.3371\tAccuracy: 0.8873\tTime: 3.78 sec\n",
      "Train #133\tLoss: 0.2889\tAccuracy: 0.9030\tTime: 69.00 sec\n",
      "Test  #133\tLoss: 0.3372\tAccuracy: 0.8879\tTime: 3.78 sec\n",
      "Train #134\tLoss: 0.2881\tAccuracy: 0.9031\tTime: 69.01 sec\n",
      "Test  #134\tLoss: 0.3356\tAccuracy: 0.8879\tTime: 3.78 sec\n",
      "Train #135\tLoss: 0.2868\tAccuracy: 0.9040\tTime: 69.03 sec\n",
      "Test  #135\tLoss: 0.3368\tAccuracy: 0.8878\tTime: 3.79 sec\n",
      "Train #136\tLoss: 0.2860\tAccuracy: 0.9038\tTime: 69.00 sec\n",
      "Test  #136\tLoss: 0.3353\tAccuracy: 0.8883\tTime: 3.77 sec\n",
      "Train #137\tLoss: 0.2848\tAccuracy: 0.9042\tTime: 69.17 sec\n",
      "Test  #137\tLoss: 0.3334\tAccuracy: 0.8888\tTime: 3.78 sec\n",
      "Train #138\tLoss: 0.2840\tAccuracy: 0.9046\tTime: 69.03 sec\n",
      "Test  #138\tLoss: 0.3331\tAccuracy: 0.8900\tTime: 3.78 sec\n",
      "Train #139\tLoss: 0.2831\tAccuracy: 0.9046\tTime: 69.01 sec\n",
      "Test  #139\tLoss: 0.3319\tAccuracy: 0.8901\tTime: 3.80 sec\n",
      "Train #140\tLoss: 0.2821\tAccuracy: 0.9053\tTime: 69.04 sec\n",
      "Test  #140\tLoss: 0.3330\tAccuracy: 0.8898\tTime: 3.79 sec\n",
      "Train #141\tLoss: 0.2811\tAccuracy: 0.9056\tTime: 68.96 sec\n",
      "Test  #141\tLoss: 0.3310\tAccuracy: 0.8898\tTime: 3.78 sec\n",
      "Train #142\tLoss: 0.2800\tAccuracy: 0.9056\tTime: 69.02 sec\n",
      "Test  #142\tLoss: 0.3294\tAccuracy: 0.8902\tTime: 3.77 sec\n",
      "Train #143\tLoss: 0.2789\tAccuracy: 0.9059\tTime: 69.05 sec\n",
      "Test  #143\tLoss: 0.3293\tAccuracy: 0.8917\tTime: 3.78 sec\n",
      "Train #144\tLoss: 0.2779\tAccuracy: 0.9064\tTime: 69.11 sec\n",
      "Test  #144\tLoss: 0.3270\tAccuracy: 0.8921\tTime: 3.79 sec\n",
      "Train #145\tLoss: 0.2774\tAccuracy: 0.9066\tTime: 69.04 sec\n",
      "Test  #145\tLoss: 0.3251\tAccuracy: 0.8925\tTime: 3.78 sec\n",
      "Train #146\tLoss: 0.2763\tAccuracy: 0.9072\tTime: 68.95 sec\n",
      "Test  #146\tLoss: 0.3244\tAccuracy: 0.8927\tTime: 3.79 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train #147\tLoss: 0.2754\tAccuracy: 0.9075\tTime: 69.04 sec\n",
      "Test  #147\tLoss: 0.3241\tAccuracy: 0.8923\tTime: 3.80 sec\n",
      "Train #148\tLoss: 0.2750\tAccuracy: 0.9077\tTime: 69.22 sec\n",
      "Test  #148\tLoss: 0.3259\tAccuracy: 0.8919\tTime: 3.77 sec\n",
      "Train #149\tLoss: 0.2744\tAccuracy: 0.9081\tTime: 69.11 sec\n",
      "Test  #149\tLoss: 0.3231\tAccuracy: 0.8930\tTime: 3.77 sec\n",
      "Train #150\tLoss: 0.2736\tAccuracy: 0.9080\tTime: 69.04 sec\n",
      "Test  #150\tLoss: 0.3241\tAccuracy: 0.8924\tTime: 3.78 sec\n",
      "Train #151\tLoss: 0.2724\tAccuracy: 0.9085\tTime: 69.05 sec\n",
      "Test  #151\tLoss: 0.3238\tAccuracy: 0.8929\tTime: 3.78 sec\n",
      "Train #152\tLoss: 0.2714\tAccuracy: 0.9087\tTime: 69.00 sec\n",
      "Test  #152\tLoss: 0.3235\tAccuracy: 0.8927\tTime: 3.78 sec\n",
      "Train #153\tLoss: 0.2710\tAccuracy: 0.9090\tTime: 69.08 sec\n",
      "Test  #153\tLoss: 0.3252\tAccuracy: 0.8929\tTime: 3.77 sec\n",
      "Train #154\tLoss: 0.2699\tAccuracy: 0.9092\tTime: 68.99 sec\n",
      "Test  #154\tLoss: 0.3231\tAccuracy: 0.8930\tTime: 3.78 sec\n",
      "Train #155\tLoss: 0.2688\tAccuracy: 0.9092\tTime: 69.02 sec\n",
      "Test  #155\tLoss: 0.3242\tAccuracy: 0.8935\tTime: 3.78 sec\n",
      "Train #156\tLoss: 0.2684\tAccuracy: 0.9097\tTime: 68.91 sec\n",
      "Test  #156\tLoss: 0.3258\tAccuracy: 0.8932\tTime: 3.79 sec\n",
      "Train #157\tLoss: 0.2679\tAccuracy: 0.9100\tTime: 69.15 sec\n",
      "Test  #157\tLoss: 0.3255\tAccuracy: 0.8930\tTime: 3.78 sec\n",
      "Train #158\tLoss: 0.2671\tAccuracy: 0.9101\tTime: 69.23 sec\n",
      "Test  #158\tLoss: 0.3251\tAccuracy: 0.8932\tTime: 3.77 sec\n",
      "Train #159\tLoss: 0.2664\tAccuracy: 0.9105\tTime: 69.01 sec\n",
      "Test  #159\tLoss: 0.3264\tAccuracy: 0.8939\tTime: 3.77 sec\n",
      "Train #160\tLoss: 0.2658\tAccuracy: 0.9104\tTime: 69.01 sec\n",
      "Test  #160\tLoss: 0.3221\tAccuracy: 0.8953\tTime: 3.78 sec\n",
      "Train #161\tLoss: 0.2650\tAccuracy: 0.9110\tTime: 68.92 sec\n",
      "Test  #161\tLoss: 0.3205\tAccuracy: 0.8970\tTime: 3.78 sec\n",
      "Train #162\tLoss: 0.2641\tAccuracy: 0.9113\tTime: 68.99 sec\n",
      "Test  #162\tLoss: 0.3203\tAccuracy: 0.8972\tTime: 3.78 sec\n",
      "Train #163\tLoss: 0.2634\tAccuracy: 0.9113\tTime: 69.02 sec\n",
      "Test  #163\tLoss: 0.3188\tAccuracy: 0.8974\tTime: 3.80 sec\n",
      "Train #164\tLoss: 0.2627\tAccuracy: 0.9114\tTime: 69.00 sec\n",
      "Test  #164\tLoss: 0.3194\tAccuracy: 0.8977\tTime: 3.78 sec\n",
      "Train #165\tLoss: 0.2621\tAccuracy: 0.9118\tTime: 68.94 sec\n",
      "Test  #165\tLoss: 0.3208\tAccuracy: 0.8964\tTime: 3.77 sec\n",
      "Train #166\tLoss: 0.2614\tAccuracy: 0.9122\tTime: 68.99 sec\n",
      "Test  #166\tLoss: 0.3216\tAccuracy: 0.8959\tTime: 3.77 sec\n",
      "Train #167\tLoss: 0.2605\tAccuracy: 0.9124\tTime: 69.05 sec\n",
      "Test  #167\tLoss: 0.3257\tAccuracy: 0.8946\tTime: 3.77 sec\n",
      "Train #168\tLoss: 0.2597\tAccuracy: 0.9127\tTime: 69.03 sec\n",
      "Test  #168\tLoss: 0.3227\tAccuracy: 0.8962\tTime: 3.80 sec\n",
      "Train #169\tLoss: 0.2586\tAccuracy: 0.9132\tTime: 69.01 sec\n",
      "Test  #169\tLoss: 0.3227\tAccuracy: 0.8954\tTime: 3.78 sec\n",
      "Train #170\tLoss: 0.2582\tAccuracy: 0.9136\tTime: 69.21 sec\n",
      "Test  #170\tLoss: 0.3239\tAccuracy: 0.8951\tTime: 3.80 sec\n",
      "Train #171\tLoss: 0.2570\tAccuracy: 0.9140\tTime: 69.03 sec\n",
      "Test  #171\tLoss: 0.3200\tAccuracy: 0.8964\tTime: 3.78 sec\n",
      "Train #172\tLoss: 0.2565\tAccuracy: 0.9140\tTime: 69.17 sec\n",
      "Test  #172\tLoss: 0.3209\tAccuracy: 0.8973\tTime: 3.79 sec\n",
      "Train #173\tLoss: 0.2558\tAccuracy: 0.9137\tTime: 69.20 sec\n",
      "Test  #173\tLoss: 0.3219\tAccuracy: 0.8962\tTime: 3.78 sec\n",
      "Train #174\tLoss: 0.2550\tAccuracy: 0.9143\tTime: 69.11 sec\n",
      "Test  #174\tLoss: 0.3206\tAccuracy: 0.8968\tTime: 3.78 sec\n",
      "Train #175\tLoss: 0.2543\tAccuracy: 0.9146\tTime: 69.10 sec\n",
      "Test  #175\tLoss: 0.3249\tAccuracy: 0.8960\tTime: 3.79 sec\n",
      "Train #176\tLoss: 0.2538\tAccuracy: 0.9152\tTime: 69.07 sec\n",
      "Test  #176\tLoss: 0.3202\tAccuracy: 0.8955\tTime: 3.78 sec\n",
      "Train #177\tLoss: 0.2529\tAccuracy: 0.9153\tTime: 69.00 sec\n",
      "Test  #177\tLoss: 0.3243\tAccuracy: 0.8953\tTime: 3.78 sec\n",
      "Train #178\tLoss: 0.2524\tAccuracy: 0.9152\tTime: 69.01 sec\n",
      "Test  #178\tLoss: 0.3237\tAccuracy: 0.8954\tTime: 3.77 sec\n",
      "Train #179\tLoss: 0.2518\tAccuracy: 0.9156\tTime: 69.03 sec\n",
      "Test  #179\tLoss: 0.3207\tAccuracy: 0.8957\tTime: 3.78 sec\n",
      "Train #180\tLoss: 0.2511\tAccuracy: 0.9157\tTime: 69.22 sec\n",
      "Test  #180\tLoss: 0.3238\tAccuracy: 0.8956\tTime: 3.77 sec\n",
      "Train #181\tLoss: 0.2507\tAccuracy: 0.9159\tTime: 69.05 sec\n",
      "Test  #181\tLoss: 0.3261\tAccuracy: 0.8953\tTime: 3.78 sec\n",
      "Train #182\tLoss: 0.2498\tAccuracy: 0.9164\tTime: 69.08 sec\n",
      "Test  #182\tLoss: 0.3220\tAccuracy: 0.8974\tTime: 3.79 sec\n",
      "Train #183\tLoss: 0.2492\tAccuracy: 0.9165\tTime: 69.07 sec\n",
      "Test  #183\tLoss: 0.3202\tAccuracy: 0.8975\tTime: 3.78 sec\n",
      "Train #184\tLoss: 0.2485\tAccuracy: 0.9171\tTime: 69.26 sec\n",
      "Test  #184\tLoss: 0.3163\tAccuracy: 0.8996\tTime: 3.78 sec\n",
      "Train #185\tLoss: 0.2478\tAccuracy: 0.9173\tTime: 69.07 sec\n",
      "Test  #185\tLoss: 0.3172\tAccuracy: 0.8987\tTime: 3.78 sec\n",
      "Train #186\tLoss: 0.2470\tAccuracy: 0.9173\tTime: 68.94 sec\n",
      "Test  #186\tLoss: 0.3203\tAccuracy: 0.8974\tTime: 3.78 sec\n",
      "Train #187\tLoss: 0.2464\tAccuracy: 0.9175\tTime: 69.12 sec\n",
      "Test  #187\tLoss: 0.3129\tAccuracy: 0.8993\tTime: 3.79 sec\n",
      "Train #188\tLoss: 0.2459\tAccuracy: 0.9178\tTime: 69.06 sec\n",
      "Test  #188\tLoss: 0.3153\tAccuracy: 0.8990\tTime: 3.78 sec\n",
      "Train #189\tLoss: 0.2451\tAccuracy: 0.9176\tTime: 69.18 sec\n",
      "Test  #189\tLoss: 0.3048\tAccuracy: 0.9014\tTime: 3.79 sec\n",
      "Train #190\tLoss: 0.2444\tAccuracy: 0.9182\tTime: 69.04 sec\n",
      "Test  #190\tLoss: 0.3014\tAccuracy: 0.9023\tTime: 3.78 sec\n",
      "Train #191\tLoss: 0.2439\tAccuracy: 0.9186\tTime: 68.96 sec\n",
      "Test  #191\tLoss: 0.3050\tAccuracy: 0.9011\tTime: 3.78 sec\n",
      "Train #192\tLoss: 0.2432\tAccuracy: 0.9185\tTime: 69.05 sec\n",
      "Test  #192\tLoss: 0.2982\tAccuracy: 0.9037\tTime: 3.77 sec\n",
      "Train #193\tLoss: 0.2427\tAccuracy: 0.9185\tTime: 68.94 sec\n",
      "Test  #193\tLoss: 0.3046\tAccuracy: 0.9016\tTime: 3.78 sec\n",
      "Train #194\tLoss: 0.2421\tAccuracy: 0.9188\tTime: 68.90 sec\n",
      "Test  #194\tLoss: 0.3005\tAccuracy: 0.9033\tTime: 3.78 sec\n",
      "Train #195\tLoss: 0.2415\tAccuracy: 0.9192\tTime: 68.99 sec\n",
      "Test  #195\tLoss: 0.2987\tAccuracy: 0.9042\tTime: 3.79 sec\n",
      "Train #196\tLoss: 0.2410\tAccuracy: 0.9193\tTime: 69.04 sec\n",
      "Test  #196\tLoss: 0.2978\tAccuracy: 0.9037\tTime: 3.85 sec\n",
      "Train #197\tLoss: 0.2403\tAccuracy: 0.9196\tTime: 69.20 sec\n",
      "Test  #197\tLoss: 0.2975\tAccuracy: 0.9029\tTime: 3.78 sec\n",
      "Train #198\tLoss: 0.2400\tAccuracy: 0.9196\tTime: 68.91 sec\n",
      "Test  #198\tLoss: 0.2946\tAccuracy: 0.9054\tTime: 3.77 sec\n",
      "Train #199\tLoss: 0.2392\tAccuracy: 0.9200\tTime: 69.06 sec\n",
      "Test  #199\tLoss: 0.2953\tAccuracy: 0.9050\tTime: 3.78 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batch_size = 40\n",
    "n_epoch = 200\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # Train\n",
    "    avg_loss, acc, run_time = run_net(X_train, y_train, batch_size,\n",
    "                                      dnn, is_train=True)\n",
    "    print(f'Train #{epoch}\\tLoss: {avg_loss:.4f}\\t'\n",
    "          f'Accuracy: {acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "    # Test\n",
    "    avg_loss, acc, run_time = run_net(X_test, y_test, batch_size,\n",
    "                                      dnn, is_train=False)\n",
    "    print(f'Test  #{epoch}\\tLoss: {avg_loss:.4f}\\t'\n",
    "          f'Accuracy: {acc:.4f}\\tTime: {run_time:.2f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-surveillance",
   "metadata": {},
   "source": [
    "As we can see this model has achieved 90+% of accuracy on these long sequences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
