{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "different-thinking",
   "metadata": {},
   "source": [
    "Copyright Â© 2017-2021 ABBYY Production LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustainable-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-alert",
   "metadata": {},
   "source": [
    "# Identity recurrent neural network (IRNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-vector",
   "metadata": {},
   "source": [
    "[Download the tutorial as a Jupyter notebook](https://github.com/neoml-lib/neoml/blob/master/NeoML/docs/en/Python/tutorials/IRNN.ipynb)\n",
    "\n",
    "In this tutorial, we'll demonstrate that an identity recurrent neural network (IRNN) can efficiently process long temporal sequences, reproducing one of the experiments described in the [Identity RNN article](https://arxiv.org/pdf/1504.00941.pdf). \n",
    "\n",
    "The experiment tests the IRNN on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, first transforming its 28 x 28 images into 784-pixel-long sequences. The article claims that IRNN can achieve 0.9+ accuracy in these conditions.\n",
    "\n",
    "The tutorial includes the following steps:\n",
    "\n",
    "* [Download and prepare the dataset](#Download-and-prepare-the-dataset)\n",
    "* [Build the network](#Build-the-network)\n",
    "* [Train the network and evaluate the results](#Train-the-network-and-evaluate-the-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-essex",
   "metadata": {},
   "source": [
    "## Download and prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-magnet",
   "metadata": {},
   "source": [
    "We will download the MNIST dataset from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aggregate-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-convergence",
   "metadata": {},
   "source": [
    "Now we need to normalize it and convert to 32-bit datatypes for NeoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize\n",
    "X = (255 - X) * 2 / 255 - 1\n",
    "\n",
    "# Fix data types\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-overall",
   "metadata": {},
   "source": [
    "Finally, we'll split the data into subsets used for training and for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confused-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "train_size = 60000\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-affiliate",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-latex",
   "metadata": {},
   "source": [
    "### Choose the device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-forwarding",
   "metadata": {},
   "source": [
    "We need to create a math engine that will perform all calculations and allocate data for the neural network. The math engine is tied to the processing device.\n",
    "\n",
    "In this tutorial we'll use a single-threaded CPU math engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mature-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neoml\n",
    "\n",
    "math_engine = neoml.MathEngine.CpuMathEngine(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-bouquet",
   "metadata": {},
   "source": [
    "### Create the network and connect layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-mambo",
   "metadata": {},
   "source": [
    "Create a `neoml.Dnn.Dnn` object that represents a neural network (a directed graph of layers). The network requires a math engine to perform its operations; it must be specified at creation and can't be changed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worthy-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = neoml.Dnn.Dnn(math_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-participation",
   "metadata": {},
   "source": [
    "A `neoml.Dnn.Source` layer feeds the data into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forty-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = neoml.Dnn.Source(dnn, 'data')  # source for data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-bloom",
   "metadata": {},
   "source": [
    "Now we need to transpose this data into sequences of 784 pixels each. We can do that using the `neoml.Dnn.Transpose` layer, which swaps 2 dimensions of the blob.\n",
    "\n",
    "Original data will be wrapped into a 2-dimensional blob with `BatchWidth` equal to batch size and `Channels` equal to image size. (We're creating blobs before training the network, [see below](#Train-the-network-and-evaluate-the-results).) This layer will transform it into sequences (`BatchLength`) of image size, where each element of the sequence will be of size `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "square-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose = neoml.Dnn.Transpose(data, first_dim='batch_length',\n",
    "                                second_dim='channels', name='transpose')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-chinese",
   "metadata": {},
   "source": [
    "We add the `neoml.Dnn.Irnn` layer, connecting its input to the output of the transposition layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "whole-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "irnn = neoml.Dnn.Irnn(transpose, hidden_size, identity_scale=1.,\n",
    "                      input_weight_std=1e-3, name='irnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-bubble",
   "metadata": {},
   "source": [
    "But recurrent layers in NeoML usually return whole sequences. To reproduce the experiment, we only need the last element of each. The `neoml.Dnn.SubSequence` layer will help us there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "large-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "subseq = neoml.Dnn.SubSequence(irnn, start_pos=-1,\n",
    "                               length=1, name='subseq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-joint",
   "metadata": {},
   "source": [
    "Now we use a fully-connected layer to form logits (non-normalized distribution) over MNIST classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adaptive-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "fc = neoml.Dnn.FullyConnected(subseq, n_classes, name='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-harbor",
   "metadata": {},
   "source": [
    "To train the network, we also need to define a loss function to be optimized. In this tutorial we'll be optimizing cross-entropy loss.\n",
    "\n",
    "A loss function needs to compare the network output with the correct labels, so we'll add another source layer to pass the correct labels in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accurate-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = neoml.Dnn.Source(dnn, 'labels')  # Source for labels\n",
    "loss = neoml.Dnn.CrossEntropyLoss((fc, labels), name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-majority",
   "metadata": {},
   "source": [
    "NeoML also provides a `neoml.Dnn.Accuracy` layer to calculate network accuracy. Let's connect this layer and create an additional `neoml.Dnn.Sink` layer for extracting its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wicked-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilary layers in order to get statistics\n",
    "accuracy = neoml.Dnn.Accuracy((fc, labels), name='accuracy')\n",
    "# accuracy layers writes its result to its output\n",
    "# We need additional sink layer to extract it\n",
    "accuracy_sink = neoml.Dnn.Sink(accuracy, name='accuracy_sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-january",
   "metadata": {},
   "source": [
    "### Create a solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-summer",
   "metadata": {},
   "source": [
    "Solver is an object that optimizes the weights using gradient values. It is necessary for training the network. In this sample we'll use a `neoml.Dnn.AdaptiveGradient` solver, which is the NeoML implementation of [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vocational-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-6\n",
    "\n",
    "# Create solver\n",
    "dnn.solver = neoml.Dnn.AdaptiveGradient(math_engine, learning_rate=lr,\n",
    "                                           l1=0., l2=0.,  # no regularization\n",
    "                                           max_gradient_norm=1.,  # clip gradients\n",
    "                                           moment_decay_rate=0.9,\n",
    "                                           second_moment_decay_rate=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-fields",
   "metadata": {},
   "source": [
    "## Train the network and evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-modification",
   "metadata": {},
   "source": [
    "NeoML networks accept data only as `neoml.Blob.Blob`.\n",
    "\n",
    "Blobs are 7-dimensional arrays located in device memory. Each dimension has a specific purpose:\n",
    "\n",
    "1. `BatchLength` - temporal axis (used in recurrent layers)\n",
    "2. `BatchWidth` - classic batch\n",
    "3. `ListSize` - list axis, used when objects are related to the same entity, but without ordering (unlike `BatchLength`)\n",
    "4. `Height` - height of the image\n",
    "5. `Width` - width of the image\n",
    "6. `Depth` - depth of the 3-dimensional image\n",
    "7. `Channels` - channels of the image (also used when object is a 1-dimensional vector)\n",
    "\n",
    "We will use `ndarray` to split data into batches, then create blobs from these batches right before feeding them into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "commercial-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irnn_data_iterator(X, y, batch_size, math_engine):\n",
    "    \"\"\"Slices numpy arrays into batches and wraps them in blobs\"\"\"\n",
    "    def make_blob(data, math_engine):\n",
    "        \"\"\"Wraps numpy data into neoml blob\"\"\"\n",
    "        shape = data.shape\n",
    "        if len(shape) == 2:  # data\n",
    "            # Wrap 2-D array into blob of (BatchWidth, Channels) shape\n",
    "            return neoml.Blob.asblob(math_engine, data,\n",
    "                                     (1, shape[0], 1, 1, 1, 1, shape[1]))\n",
    "        elif len(shape) == 1:  # dense labels\n",
    "            # Wrap 1-D array into blob of (BatchWidth,) shape\n",
    "            return neoml.Blob.asblob(math_engine, data,\n",
    "                                     (1, shape[0], 1, 1, 1, 1, 1))\n",
    "        else:\n",
    "            assert(False)\n",
    "\n",
    "    start = 0\n",
    "    data_size = y.shape[0]\n",
    "    while start < data_size:\n",
    "        yield (make_blob(X[start : start+batch_size], math_engine),\n",
    "               make_blob(y[start : start+batch_size], math_engine))\n",
    "        start += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-tomorrow",
   "metadata": {},
   "source": [
    "To train the network, call `dnn.learn` with data as its argument.\n",
    "\n",
    "To run the network without training, call `dnn.run` with data as its argument.\n",
    "\n",
    "The input data is a `dict` where each key is a `neoml.Dnn.Source` layer name and the corresponding value is the `neoml.Blob.Blob` that should be passed in to this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "brazilian-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net(X, y, batch_size, dnn, is_train):\n",
    "    \"\"\"Runs dnn on given data\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0.\n",
    "    run_iter = dnn.learn if is_train else dnn.run\n",
    "    math_engine = dnn.math_engine\n",
    "    layers = dnn.layers\n",
    "    loss = layers['loss']\n",
    "    accuracy = layers['accuracy']\n",
    "    sink = layers['accuracy_sink']\n",
    "\n",
    "    accuracy.reset = True  # Reset previous statistics\n",
    "    # Iterate over batches\n",
    "    for X_batch, y_batch in irnn_data_iterator(X, y, batch_size, math_engine):\n",
    "        # Run the network on the batch data\n",
    "        run_iter({'data': X_batch, 'labels': y_batch})\n",
    "        total_loss += loss.last_loss * y_batch.batch_width  # Update epoch loss\n",
    "        accuracy.reset = False  # Don't reset statistics within one epoch\n",
    "\n",
    "    avg_loss = total_loss / y.shape[0]\n",
    "    avg_acc = sink.get_blob().asarray()[0]\n",
    "    run_time = time.time() - start\n",
    "    return avg_loss, avg_acc, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-mexican",
   "metadata": {},
   "source": [
    "*Note*: It will take 3-4 hours to train. You may uncomment print statements to see the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acute-affiliation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test acc: 0.9050\n",
      "Wall time: 3h 54min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "batch_size = 40\n",
    "n_epoch = 200\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    # Train\n",
    "    train_loss, train_acc, run_time = run_net(X_train, y_train, batch_size,\n",
    "                                      dnn, is_train=True)\n",
    "    # print(f'Train #{epoch}\\tLoss: {train_loss:.4f}\\t'\n",
    "    #       f'Accuracy: {train_acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "    # Test\n",
    "    test_loss, test_acc, run_time = run_net(X_test, y_test, batch_size,\n",
    "                                      dnn, is_train=False)\n",
    "    # print(f'Test  #{epoch}\\tLoss: {test_loss:.4f}\\t'\n",
    "    #       f'Accuracy: {test_acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "print(f'Final test acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-surveillance",
   "metadata": {},
   "source": [
    "As we can see, this model actually has achieved 0.9+ accuracy on these long sequences, confirming the paper's results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
