{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disabled-singing",
   "metadata": {},
   "source": [
    "Copyright Â© 2017-2021 ABBYY Production LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interstate-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-bahrain",
   "metadata": {},
   "source": [
    "# Cifar-10 neural net tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-banking",
   "metadata": {},
   "source": [
    "This tutorial contains the following steps:\n",
    "\n",
    "* Download CIFAR-10 dataset\n",
    "* Prepare CIFAR-10 dataset\n",
    "* Build the net for training\n",
    "* Train the net on the dataset\n",
    "* Prepare the net for inference\n",
    "* Evaluate the net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-collins",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-length",
   "metadata": {},
   "source": [
    "*Note*: This section doesn't have any NeoML-specific code. It just downloads dataset from the internet. If you are not running this notebook, you may [skip](#Prepare-data) this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pacific-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def calc_md5(file_name):\n",
    "    \"\"\"Calculates md5 hash of an existing file\"\"\"\n",
    "    import hashlib\n",
    "    curr_hash = hashlib.md5()\n",
    "    with open(file_name, 'rb') as file_in:\n",
    "        chunk = file_in.read(8192)\n",
    "        while chunk:\n",
    "            curr_hash.update(chunk)\n",
    "            chunk = file_in.read(8192)\n",
    "    return curr_hash.hexdigest()\n",
    "\n",
    "\n",
    "# Download data\n",
    "url = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "file_name = url[url.rfind('/')+1:]\n",
    "ARCHIVE_SIZE = 170498071\n",
    "ARCHIVE_MD5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "\n",
    "# Download when archive is missing or broken\n",
    "if (not os.path.isfile(file_name)) \\\n",
    "        or os.path.getsize(file_name) != ARCHIVE_SIZE \\\n",
    "        or calc_md5(file_name) != ARCHIVE_MD5:\n",
    "    import requests\n",
    "    with requests.get(url, stream=True) as url_stream:\n",
    "        url_stream.raise_for_status()\n",
    "        with open(file_name, 'wb') as file_out:\n",
    "            for chunk in url_stream.iter_content(chunk_size=8192):\n",
    "                file_out.write(chunk)\n",
    "\n",
    "# Unpack data\n",
    "import tarfile\n",
    "tar = tarfile.open(file_name, 'r:gz')\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-search",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-excellence",
   "metadata": {},
   "source": [
    "In this section data is loaded from files into numpy arrays and pre-processed. Pre-processing includes the following:\n",
    "\n",
    "* Data type conversion (NeoML works with 32-bit types for both integer and float data)\n",
    "* Normalization\n",
    "* Image format conversion (NeoML works with channel-last images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sophisticated-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(666)\n",
    "\n",
    "\n",
    "def load_batch_file(file_name):\n",
    "    \"\"\"Loads data from one of the batch files\"\"\"\n",
    "    import pickle\n",
    "    with open(file_name, 'rb') as file_in:\n",
    "        result = pickle.load(file_in, encoding='bytes')\n",
    "    return result\n",
    "\n",
    "\n",
    "def transform_data(X):\n",
    "    \"\"\"Normalizes and transposes data for NeoML\"\"\"\n",
    "    X = X.astype(np.float32)\n",
    "    X = (X - 127.5) / 255.\n",
    "    X = X.reshape((X.shape[0], 3, 32, 32))\n",
    "    X = X.transpose((0, 2, 3, 1))  # NeoML uses channel-last pack\n",
    "    return X\n",
    "\n",
    "\n",
    "# Preparing data\n",
    "batch_name = 'cifar-10-batches-py/data_batch_{0}'\n",
    "train_data = [load_batch_file(batch_name.format(i)) for i in range(1, 6)]\n",
    "X_train = np.concatenate(list(x[b'data'] for x in train_data), axis=0)\n",
    "X_train = transform_data(X_train)\n",
    "y_train = np.concatenate(list(x[b'labels'] for x in train_data), axis=0)\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "test_data = load_batch_file('cifar-10-batches-py/test_batch')\n",
    "X_test = test_data[b'data']\n",
    "X_test = transform_data(X_test)\n",
    "y_test = np.array(test_data[b'labels'], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-crown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-routine",
   "metadata": {},
   "source": [
    "### Create math engine (choose device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-opera",
   "metadata": {},
   "source": [
    "First of all we need to create math engine. It's an entity responsible for computational operations and data allocation for neural networks. It determines the device used for neural network training and inference.\n",
    "\n",
    "For faster training in this tutorial we'll use GPU.\n",
    "\n",
    "*Note:* If NeoML won't manage to find any compatible GPU, it'll create CPU math engine. You may check which math engine was created by looking at its `info` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alert-queue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  CUDA: GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import neoml\n",
    "\n",
    "# If you wanna use CPU, you should use neoml.MathEngine.CpuMathEngine()\n",
    "math_engine = neoml.MathEngine.GpuMathEngine(0)\n",
    "print('Device: ', math_engine.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-hamburg",
   "metadata": {},
   "source": [
    "### Build the DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-level",
   "metadata": {},
   "source": [
    "First of all we need the `neoml.Dnn.Dnn` object which represents a neural network (a graph of layers). Every net needs a math engine to perform its operations and its math engine can't be changed after creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "characteristic-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = neoml.Dnn.Dnn(math_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-buffer",
   "metadata": {},
   "source": [
    "The data is feeded to the network via special `neoml.Dnn.Source` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dressed-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = neoml.Dnn.Source(dnn, 'data')  # Source for data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-steps",
   "metadata": {},
   "source": [
    "The net in this tutorial will be consisting of a few convolutional blocks. Here you may see how layers can be connected to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "manual-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock:\n",
    "    \"\"\"Block of dropout->conv->batch_norm->relu6\"\"\"\n",
    "    def __init__(self, inputs, filter_count, name):\n",
    "        self.dropout = neoml.Dnn.Dropout(inputs, rate=0.1, spatial=True,\n",
    "                                         batchwise=True, name=name+'_dropout')\n",
    "        self.conv = neoml.Dnn.Conv(self.dropout, filter_count=filter_count,\n",
    "                                   filter_size=(3, 3), stride_size=(2, 2),\n",
    "                                   padding_size=(1, 1), name=name+'_conv')\n",
    "        self.bn = neoml.Dnn.BatchNormalization(self.conv, channel_based=True,\n",
    "                                               name=name+'_bn')\n",
    "        self.output = neoml.Dnn.ReLU(self.bn, threshold=6., name=name+'_relu6')\n",
    "\n",
    "\n",
    "# Add a few convolutional blocks\n",
    "# First convolutional block takes Source layer's data as input\n",
    "block1 = ConvBlock(data, filter_count=16, name='block1')  # -> (16,  16)\n",
    "# Next convolutional blocks take as input the output of previous block\n",
    "block2 = ConvBlock(block1.output, filter_count=32, name='block2')  # -> (8, 8)\n",
    "block3 = ConvBlock(block2.output, filter_count=64, name='block3')  # -> (4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-dating",
   "metadata": {},
   "source": [
    "Afterwards we'll use fully-connected layer to generate logits (non-normalized probabilities) over classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "golden-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected flattens its input automatically\n",
    "n_classes = 10  # Number of classes in CIFAR-10 dataset\n",
    "fc = neoml.Dnn.FullyConnected(block3.output, n_classes, name='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-convergence",
   "metadata": {},
   "source": [
    "In order to train net we need to define the loss function. In NeoML it should be done by adding one (or more) loss layers. In this tutorial we'll be optimizing cross-entropy loss.\n",
    "\n",
    "*Note*: in case of multiple loss layers you may use `neoml.Dnn.Loss.loss_weight` properties to balance loss layers between each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "strategic-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before loss layer itself we need to create source layer for correct labels\n",
    "labels = neoml.Dnn.Source(dnn, 'labels')\n",
    "# Here you can see how to create a layer with multiple inputs\n",
    "# Softmax will be applied within cross-entropy (no need for explicit softmax layer here)\n",
    "loss = neoml.Dnn.CrossEntropyLoss((fc, labels), name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-hardwood",
   "metadata": {},
   "source": [
    "We'll be calculating accuracy by the means of NeoML. To do this we'll need special `neoml.Dnn.Accuracy` layer (and `neoml.Dnn.Sink` layer for extracting accuracy's output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "related-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxilary layers in order to get statistics\n",
    "accuracy = neoml.Dnn.Accuracy((fc, labels), name='accuracy')\n",
    "# accuracy layers writes its result to its output\n",
    "# We need additional sink layer to extract it\n",
    "accuracy_sink = neoml.Dnn.Sink(accuracy, name='accuracy_sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-founder",
   "metadata": {},
   "source": [
    "### Create solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-joseph",
   "metadata": {},
   "source": [
    "Solver is an object which is responsible for weight optimization (based on gradient values). In this sample we'll use `neoml.Dnn.AdaptiveGradient` solver (neoml's realization of [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hundred-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # Learning rate\n",
    "\n",
    "# Create solver\n",
    "dnn.solver = neoml.Dnn.AdaptiveGradient(math_engine, learning_rate=lr,\n",
    "                                        l1=0., l2=0.,  # No regularization\n",
    "                                        max_gradient_norm=1.,  # clip grad\n",
    "                                        moment_decay_rate=0.9,\n",
    "                                        second_moment_decay_rate=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-belief",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-lafayette",
   "metadata": {},
   "source": [
    "The neoml's nets accept data only as `neoml.Blob.Blob`.\n",
    "\n",
    "Blobs are 7-dimensional arrays located in device memory. Each dimension has a specific purpose:\n",
    "\n",
    "1. `BatchLength` - temporal axis (used in recurrent layers)\n",
    "2. `BatchWidth` - classic batch\n",
    "3. `ListSize` - list axis, used when objects are related to the same entity, but without ordering (unlike `BatchLength`)\n",
    "4. `Height` - height of the image\n",
    "5. `Width` - width of the image\n",
    "6. `Depth` - depth of the 3-dimensional image\n",
    "7. `Channels` - channels of the image (also used when object is a 1-dimensional vector)\n",
    "\n",
    "In our case we will use `ndarray` to split data into batches. Blobs will be created based on these batches right before feeding them to the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "written-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_blob(data, math_engine):\n",
    "    \"\"\"Wraps numpy data into neoml blob\"\"\"\n",
    "    shape = data.shape\n",
    "    if len(shape) == 4:  # images\n",
    "        # Data is a batch of 2-dimensional multi-channel images\n",
    "        # Wrap it into (BatchWidth, Height, Width, Channels) blob\n",
    "        blob_shape = (1, shape[0], 1, shape[1], shape[2], 1, shape[3])\n",
    "        return neoml.Blob.asblob(math_engine, data, blob_shape)\n",
    "    elif len(shape) == 1:  # dense labels\n",
    "        # Data contains dense labels (batch of integers)\n",
    "        # Wrap it into blob of (BatchWidth,) shape\n",
    "        return neoml.Blob.asblob(math_engine, data,\n",
    "                                 (1, shape[0], 1, 1, 1, 1, 1))\n",
    "    else:\n",
    "        assert(False)\n",
    "\n",
    "\n",
    "def cifar10_array_iter(X, y, batch_size):\n",
    "    \"\"\"Slices numpy arrays into batches\"\"\"\n",
    "    start = 0\n",
    "    data_size = y.shape[0]\n",
    "    while start < data_size:\n",
    "        yield X[start : start+batch_size], y[start : start+batch_size]\n",
    "        start += batch_size\n",
    "\n",
    "\n",
    "def cifar10_blob_iter(X, y, batch_size, math_engine):\n",
    "    \"\"\"Slices numpy arrays into batches and wraps them in blobs\"\"\"\n",
    "    for X_b, y_b in cifar10_array_iter(X, y, batch_size):\n",
    "        yield make_blob(X_b, math_engine), make_blob(y_b, math_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-latest",
   "metadata": {},
   "source": [
    "In order to train net you should call `dnn.learn` with data as its argument.\n",
    "\n",
    "In order to run net without traning you should call `dnn.run` with data as its argument.\n",
    "\n",
    "The data argument is a `dict` where keys are `neoml.Dnn.Source` layers' names and values are corresponding `neoml.Blob.Blob`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "academic-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net(X, y, batch_size, dnn, is_train):\n",
    "    \"\"\"Runs dnn on given data\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0.\n",
    "    run_iter = dnn.learn if is_train else dnn.run\n",
    "    math_engine = dnn.math_engine\n",
    "    layers = dnn.layers\n",
    "    loss = layers['loss']\n",
    "    accuracy = layers['accuracy']\n",
    "    sink = layers['accuracy_sink']\n",
    "\n",
    "    accuracy.reset = True  # Reset previous statistics\n",
    "    # Iterate over batches\n",
    "    for X_batch, y_batch in cifar10_blob_iter(X, y, batch_size, math_engine):\n",
    "        # Run the network on the batch data\n",
    "        run_iter({'data': X_batch, 'labels': y_batch})\n",
    "        total_loss += loss.last_loss * y_batch.batch_width  # Update epoch loss\n",
    "        accuracy.reset = False  # Don't reset statistics within one epoch\n",
    "\n",
    "    avg_loss = total_loss / y.shape[0]\n",
    "    avg_acc = sink.get_blob().asarray()[0]\n",
    "    run_time = time.time() - start\n",
    "    return avg_loss, avg_acc, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-spouse",
   "metadata": {},
   "source": [
    "For educational purpose we store and load progress during training.\n",
    "\n",
    "The training progess can be stored by `dnn.store_checkpoint` method.\n",
    "\n",
    "You may resume training from the checkpoint by calling `dnn.load_checkpoint`.\n",
    "\n",
    "**Important**: neoml's checkpoints contain all the information required for training (*including the net architecture*). That allows us to `load_checkpoint` into any `neoml.Dnn.Dnn` object without the need to re-create architecture or solver before loading. However, this approach leads to the creation of new layer/solver/blob objects during each `dnn.load_checkpoint`. If you had any previously created python variables which were pointing to the objects of the net *before loading* (like `solver`, `data` variables here), you must re-initialize them with the new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subsequent-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train #0\tLoss: 1.5371\tAccuracy: 0.4499\tTime: 6.12 sec\n",
      "Test  #0\tLoss: 1.2951\tAccuracy: 0.5341\tTime: 0.71 sec\n",
      "Train #1\tLoss: 1.2288\tAccuracy: 0.5630\tTime: 5.87 sec\n",
      "Test  #1\tLoss: 1.1361\tAccuracy: 0.5951\tTime: 0.71 sec\n",
      "Creating checkpoint...\n",
      "Train #2\tLoss: 1.1138\tAccuracy: 0.6064\tTime: 5.90 sec\n",
      "Test  #2\tLoss: 1.2091\tAccuracy: 0.5761\tTime: 0.70 sec\n",
      "Train #3\tLoss: 1.0385\tAccuracy: 0.6321\tTime: 5.91 sec\n",
      "Test  #3\tLoss: 1.0687\tAccuracy: 0.6177\tTime: 0.71 sec\n",
      "Train #4\tLoss: 0.9907\tAccuracy: 0.6520\tTime: 5.92 sec\n",
      "Test  #4\tLoss: 1.0566\tAccuracy: 0.6293\tTime: 0.70 sec\n",
      "Train #5\tLoss: 0.9494\tAccuracy: 0.6647\tTime: 5.93 sec\n",
      "Test  #5\tLoss: 1.0407\tAccuracy: 0.6361\tTime: 0.70 sec\n",
      "Loading checkpoint... (this will roll dnn back to epoch #1)\n",
      "Train #6\tLoss: 1.1142\tAccuracy: 0.6035\tTime: 5.91 sec\n",
      "Test  #6\tLoss: 1.0858\tAccuracy: 0.6211\tTime: 0.70 sec\n",
      "Train #7\tLoss: 1.0434\tAccuracy: 0.6333\tTime: 5.95 sec\n",
      "Test  #7\tLoss: 1.1010\tAccuracy: 0.6111\tTime: 0.71 sec\n",
      "Train #8\tLoss: 0.9904\tAccuracy: 0.6490\tTime: 5.89 sec\n",
      "Test  #8\tLoss: 1.0438\tAccuracy: 0.6331\tTime: 0.71 sec\n",
      "Train #9\tLoss: 0.9491\tAccuracy: 0.6673\tTime: 5.94 sec\n",
      "Test  #9\tLoss: 1.0083\tAccuracy: 0.6476\tTime: 0.71 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Network params\n",
    "batch_size = 50\n",
    "\n",
    "n_epoch = 10\n",
    "for epoch in range(n_epoch):\n",
    "    # Train\n",
    "    avg_loss, acc, run_time = run_net(X_train, y_train, batch_size,\n",
    "                                      dnn, is_train=True)\n",
    "    print(f'Train #{epoch}\\tLoss: {avg_loss:.4f}\\t'\n",
    "          f'Accuracy: {acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "    # Test\n",
    "    avg_loss, acc, run_time = run_net(X_test, y_test, batch_size,\n",
    "                                      dnn, is_train=False)\n",
    "    print(f'Test  #{epoch}\\tLoss: {avg_loss:.4f}\\t'\n",
    "          f'Accuracy: {acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "    if epoch == 1:\n",
    "        # If you want to save training progress you can do it via checkpoints\n",
    "        # It stores dnn weights and other training data (solver stats, etc.)\n",
    "        print('Creating checkpoint...')\n",
    "        dnn.store_checkpoint('cifar10_sample.checkpoint')\n",
    "    if epoch == 5:\n",
    "        # If you want you can resume training from the checkpoint\n",
    "        print('Loading checkpoint... (this will roll dnn back to epoch #1)')\n",
    "        dnn.load_checkpoint('cifar10_sample.checkpoint')\n",
    "        # Be careful! dnn now points to the new net\n",
    "        # But other layer/solver variables still pointing to the old net!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-breed",
   "metadata": {},
   "source": [
    "## Prepare net for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-civilization",
   "metadata": {},
   "source": [
    "First of all, before inference we must delete training-only layers. Every layer requiring correct labels should be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "narrow-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove training-only layers\n",
    "dnn.delete_layer('labels')\n",
    "dnn.delete_layer('loss')\n",
    "dnn.delete_layer('accuracy')\n",
    "dnn.delete_layer('accuracy_sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-manhattan",
   "metadata": {},
   "source": [
    "But we need a sink layer to extract logits. If you need *exact* probabilities you should add `neoml.Dnn.Softmax` layer before sink. In our case we're interesting in maximum index only, that's why softmax can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rough-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sink for dnn output\n",
    "sink = neoml.Dnn.Sink(dnn.layers['fc'], name='sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-eight",
   "metadata": {},
   "source": [
    "Also we can fuse batch normalization with previous convolution or fully connected layer. That'll reduce the number of operations during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appointed-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_batch_norm(dnn, block_name):\n",
    "    \"\"\"Fuses batch_norm into convolution\n",
    "    As a result reduces inference time\n",
    "    Should be used after training\n",
    "    \"\"\"\n",
    "    bn_name = block_name + '_bn'\n",
    "    if not dnn.has_layer(bn_name):\n",
    "        # Batch norm has already been fused\n",
    "        return\n",
    "    bn_layer = dnn.layers[bn_name]\n",
    "    conv_name = block_name + '_conv'\n",
    "    conv_layer = dnn.layers[conv_name]\n",
    "    # Fuse batch normalization\n",
    "    conv_layer.apply_batch_normalization(bn_layer)\n",
    "    # Delete layer from net (conv already 'contains' it)\n",
    "    dnn.delete_layer(bn_name)\n",
    "    # Connect layer after batchnorm to convolution\n",
    "    # because batchnorm was removed from the dnn\n",
    "    output_name = block_name + '_relu6'\n",
    "    dnn.layers[output_name].connect(conv_layer)\n",
    "\n",
    "\n",
    "# Fuse batchnorms into convolutions\n",
    "fuse_batch_norm(dnn, 'block1')\n",
    "fuse_batch_norm(dnn, 'block2')\n",
    "fuse_batch_norm(dnn, 'block2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-uruguay",
   "metadata": {},
   "source": [
    "## Serialize trained net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-brooklyn",
   "metadata": {},
   "source": [
    "Use `dnn.store` to save trained. This method stores all the info required for inference. Later you may load saved net by `dnn.load` into any `neoml.Dnn.Dnn` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "damaged-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store trained net\n",
    "# In that case it's better to use method load/store\n",
    "# Unlike checkpoints those aren't working with training-related data\n",
    "# As a result they use less disk space\n",
    "dnn.store('cifar10_sample.dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "biological-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained net\n",
    "# It's not needed and done here for tutorial purpose only\n",
    "dnn.load('cifar10_sample.dnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-semester",
   "metadata": {},
   "source": [
    "Once again, `load` leads to creation of the new layer objects. And previously created `sink` variable now points to the layer object, which doesn't belong to the dnn. Let's fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "terminal-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = dnn.layers['sink']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-iceland",
   "metadata": {},
   "source": [
    "Now it's time to evaluate trained network.\n",
    "\n",
    "We get the blob with the results of the last `dnn.run` by `sink.get_blob`. Then we convert it into numpy array via `blob.asarray` and calculating accuracy by the means of numpy.\n",
    "\n",
    "We should get here the same accuracy, as during the test of the last epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "other-worth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference net test accuracy: 0.6476\n"
     ]
    }
   ],
   "source": [
    "# Evaluate inference\n",
    "inference_acc = 0.\n",
    "for X_b, y_b in cifar10_array_iter(X_test, y_test, batch_size):\n",
    "    dnn.run({'data': make_blob(X_b, math_engine)})\n",
    "    # Extract data from sink\n",
    "    # unnormalized probs of shape (batch_size, n_classes)\n",
    "    logits = sink.get_blob().asarray()\n",
    "    # Calculate accuracy\n",
    "    inference_acc += (np.argmax(logits, axis=1) == y_b).sum()\n",
    "inference_acc /= len(X_test)\n",
    "\n",
    "# This number must be equal to the test accuracy of the last epoch\n",
    "print(f'Inference net test accuracy: {inference_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
