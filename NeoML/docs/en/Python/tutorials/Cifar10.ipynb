{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disabled-singing",
   "metadata": {},
   "source": [
    "Copyright Â© 2017-2021 ABBYY Production LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interstate-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-bahrain",
   "metadata": {},
   "source": [
    "# Neural network for CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-banking",
   "metadata": {},
   "source": [
    "[Download the tutorial as a Jupyter notebook](https://github.com/neoml-lib/neoml/blob/master/NeoML/docs/en/Python/tutorials/Cifar10.ipynb)\n",
    "\n",
    "In this tutorial, we will use NeoML to create a neural network that classifies the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "The tutorial includes the following steps:\n",
    "\n",
    "* [Download the dataset](#Download-the-dataset)\n",
    "* [Prepare the dataset](#Prepare-the-dataset)\n",
    "* [Build the network](#Build-the-network)\n",
    "* [Train the network on the dataset](#Train-the-network-on-the-dataset)\n",
    "* [Prepare the network for inference](#Prepare-the-network-for-inference)\n",
    "* [Serialize the network](#Serialize-the-network)\n",
    "* [Evaluate the performance](#Evaluate-the-performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-collins",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-length",
   "metadata": {},
   "source": [
    "*Note:* This section doesn't have any NeoML-specific code. It just downloads the dataset from the internet. If you are not running this notebook, you may [skip](#Prepare-the-dataset) this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pacific-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def calc_md5(file_name):\n",
    "    \"\"\"Calculates md5 hash of an existing file\"\"\"\n",
    "    import hashlib\n",
    "    curr_hash = hashlib.md5()\n",
    "    with open(file_name, 'rb') as file_in:\n",
    "        chunk = file_in.read(8192)\n",
    "        while chunk:\n",
    "            curr_hash.update(chunk)\n",
    "            chunk = file_in.read(8192)\n",
    "    return curr_hash.hexdigest()\n",
    "\n",
    "\n",
    "# Download data\n",
    "url = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "file_name = url[url.rfind('/')+1:]\n",
    "ARCHIVE_SIZE = 170498071\n",
    "ARCHIVE_MD5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "\n",
    "# Download when archive is missing or broken\n",
    "if (not os.path.isfile(file_name)) \\\n",
    "        or os.path.getsize(file_name) != ARCHIVE_SIZE \\\n",
    "        or calc_md5(file_name) != ARCHIVE_MD5:\n",
    "    import requests\n",
    "    with requests.get(url, stream=True) as url_stream:\n",
    "        url_stream.raise_for_status()\n",
    "        with open(file_name, 'wb') as file_out:\n",
    "            for chunk in url_stream.iter_content(chunk_size=8192):\n",
    "                file_out.write(chunk)\n",
    "\n",
    "# Unpack data\n",
    "import tarfile\n",
    "tar = tarfile.open(file_name, 'r:gz')\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-search",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-excellence",
   "metadata": {},
   "source": [
    "In this section we load the data from files into numpy arrays and preprocess it. Preprocessing includes:\n",
    "\n",
    "* Data type conversion, because NeoML takes 32-bit types for both integer and float data\n",
    "* Normalization\n",
    "* Image format conversion, because NeoML works with channel-last images\n",
    "\n",
    "We'll also lump the 5 training batches of the original dataset together, because when training we'll use a much smaller batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sophisticated-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(666)\n",
    "\n",
    "def load_batch_file(file_name):\n",
    "    \"\"\"Loads data from one of the batch files\"\"\"\n",
    "    import pickle\n",
    "    with open(file_name, 'rb') as file_in:\n",
    "        result = pickle.load(file_in, encoding='bytes')\n",
    "    return result\n",
    "\n",
    "def transform_data(X):\n",
    "    \"\"\"Normalizes and transposes data for NeoML\"\"\"\n",
    "    X = X.astype(np.float32)\n",
    "    X = (X - 127.5) / 255.\n",
    "    X = X.reshape((X.shape[0], 3, 32, 32))\n",
    "    X = X.transpose((0, 2, 3, 1))  # NeoML uses channel-last pack\n",
    "    return X\n",
    "\n",
    "# Preparing data\n",
    "batch_name = 'cifar-10-batches-py/data_batch_{0}'\n",
    "train_data = [load_batch_file(batch_name.format(i)) for i in range(1, 6)]\n",
    "X_train = np.concatenate(list(x[b'data'] for x in train_data), axis=0)\n",
    "X_train = transform_data(X_train)\n",
    "y_train = np.concatenate(list(x[b'labels'] for x in train_data), axis=0)\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "test_data = load_batch_file('cifar-10-batches-py/test_batch')\n",
    "X_test = test_data[b'data']\n",
    "X_test = transform_data(X_test)\n",
    "y_test = np.array(test_data[b'labels'], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-crown",
   "metadata": {},
   "source": [
    "## Build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-routine",
   "metadata": {},
   "source": [
    "### Choose the device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-opera",
   "metadata": {},
   "source": [
    "We need to create a math engine that will perform all calculations and allocate data for the neural network. The math engine is tied to the processing device.\n",
    "\n",
    "For faster training in this tutorial we'll create the math engine that works on GPU.\n",
    "\n",
    "*Note:* If NeoML doesn't manage to find a compatible GPU, it'll create a CPU math engine. You may check which math engine was created by looking at its `info` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alert-queue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  CUDA: GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import neoml\n",
    "\n",
    "# If you'd prefer to use a CPU, call neoml.MathEngine.CpuMathEngine() instead\n",
    "math_engine = neoml.MathEngine.GpuMathEngine(0)\n",
    "print('Device: ', math_engine.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-hamburg",
   "metadata": {},
   "source": [
    "### Create the network and connect layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-level",
   "metadata": {},
   "source": [
    "Let's create a `neoml.Dnn.Dnn` object that represents a neural network (a directed graph of layers). The network requires a math engine to perform its operations; it must be specified at creation and can't be changed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "characteristic-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = neoml.Dnn.Dnn(math_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-buffer",
   "metadata": {},
   "source": [
    "A `neoml.Dnn.Source` layer feeds the data into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dressed-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = neoml.Dnn.Source(dnn, 'data')  # source for data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-steps",
   "metadata": {},
   "source": [
    "The network in this tutorial will consist of several convolutional blocks. Each block contains a dropout layer to randomly zero out some of the inputs, a convolution layer with trainable coefficients, a batch normalization layer, and a ReLU activation.\n",
    "\n",
    "Each layer gets its own name, so that it can be found if needed, and is connected to the output of the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "manual-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock:\n",
    "    \"\"\"Block of dropout->conv->batch_norm->relu6\"\"\"\n",
    "    def __init__(self, inputs, filter_count, name):\n",
    "        self.dropout = neoml.Dnn.Dropout(inputs, rate=0.1, spatial=True,\n",
    "                                         batchwise=True, name=name+'_dropout')\n",
    "        self.conv = neoml.Dnn.Conv(self.dropout, filter_count=filter_count,\n",
    "                                   filter_size=(3, 3), stride_size=(2, 2),\n",
    "                                   padding_size=(1, 1), name=name+'_conv')\n",
    "        self.bn = neoml.Dnn.BatchNormalization(self.conv, channel_based=True,\n",
    "                                               name=name+'_bn')\n",
    "        self.output = neoml.Dnn.ReLU(self.bn, threshold=6., name=name+'_relu6')\n",
    "\n",
    "\n",
    "# Add a few convolutional blocks\n",
    "# First convolutional block takes source layer's data as input\n",
    "block1 = ConvBlock(data, filter_count=16, name='block1')  # -> (16,  16)\n",
    "# Next convolutional blocks each take as input the output of the previous block\n",
    "block2 = ConvBlock(block1.output, filter_count=32, name='block2')  # -> (8, 8)\n",
    "block3 = ConvBlock(block2.output, filter_count=64, name='block3')  # -> (4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-dating",
   "metadata": {},
   "source": [
    "Afterwards we'll use a fully-connected layer to generate logits (non-normalized probabilities) over classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "golden-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully-connected layer flattens its input automatically\n",
    "n_classes = 10  # the number of classes in CIFAR-10 dataset\n",
    "fc = neoml.Dnn.FullyConnected(block3.output, n_classes, name='fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-convergence",
   "metadata": {},
   "source": [
    "To train the network, we also need to define a loss function to be optimized. In NeoML this is done by adding one or several loss layers.\n",
    "\n",
    "In this tutorial we'll be optimizing cross-entropy loss.\n",
    "\n",
    "A loss function needs to compare the network output with the correct labels, so we'll add another source layer to pass the correct labels in.\n",
    "\n",
    "*Note:* in case of multiple loss layers you may want to use `neoml.Dnn.Loss.loss_weight` properties of each layer to balance between several loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "strategic-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before loss layer itself we need to create source layer for correct labels\n",
    "labels = neoml.Dnn.Source(dnn, 'labels')\n",
    "# Here you can see how to create a layer with multiple inputs\n",
    "# Softmax will be applied within cross-entropy (no need for explicit softmax layer here)\n",
    "loss = neoml.Dnn.CrossEntropyLoss((fc, labels), name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-hardwood",
   "metadata": {},
   "source": [
    "NeoML also provides a `neoml.Dnn.Accuracy` layer to calculate network accuracy. Let's connect this layer and create an additional `neoml.Dnn.Sink` layer for extracting its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "related-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary layers needed to get statistics\n",
    "accuracy = neoml.Dnn.Accuracy((fc, labels), name='accuracy')\n",
    "# The accuracy layer writes its result to its output\n",
    "# We need additional sink layer to extract it\n",
    "accuracy_sink = neoml.Dnn.Sink(accuracy, name='accuracy_sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-founder",
   "metadata": {},
   "source": [
    "### Create a solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-joseph",
   "metadata": {},
   "source": [
    "Solver is an object that optimizes the weights using gradient values. It is necessary for training the network. In this sample we'll use a `neoml.Dnn.AdaptiveGradient` solver, which is the NeoML implementation of [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hundred-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # Learning rate\n",
    "\n",
    "# Create solver\n",
    "dnn.solver = neoml.Dnn.AdaptiveGradient(math_engine, learning_rate=lr,\n",
    "                                        l1=0., l2=0.,  # no regularization\n",
    "                                        max_gradient_norm=1.,  # clip gradients\n",
    "                                        moment_decay_rate=0.9,\n",
    "                                        second_moment_decay_rate=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-belief",
   "metadata": {},
   "source": [
    "## Train the network on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-lafayette",
   "metadata": {},
   "source": [
    "NeoML networks accept data only as `neoml.Blob.Blob`.\n",
    "\n",
    "Blobs are 7-dimensional arrays located in device memory. Each dimension has a specific purpose:\n",
    "\n",
    "1. `BatchLength` - temporal axis (used in recurrent layers)\n",
    "2. `BatchWidth` - classic batch\n",
    "3. `ListSize` - list axis, used when objects are related to the same entity, but without ordering (unlike `BatchLength`)\n",
    "4. `Height` - height of the image\n",
    "5. `Width` - width of the image\n",
    "6. `Depth` - depth of the 3-dimensional image\n",
    "7. `Channels` - channels of the image (also used when object is a 1-dimensional vector)\n",
    "\n",
    "We will use `ndarray` to split data into batches, then create blobs from these batches right before feeding them into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "written-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_blob(data, math_engine):\n",
    "    \"\"\"Wraps numpy data into a NeoML blob\"\"\"\n",
    "    shape = data.shape\n",
    "    if len(shape) == 4:  # images\n",
    "        # Data is a batch of 2-dimensional multi-channel images\n",
    "        # Wrap it into (BatchWidth, Height, Width, Channels) blob\n",
    "        blob_shape = (1, shape[0], 1, shape[1], shape[2], 1, shape[3])\n",
    "        return neoml.Blob.asblob(math_engine, data, blob_shape)\n",
    "    elif len(shape) == 1:  # dense labels\n",
    "        # Data contains dense labels (batch of integers)\n",
    "        # Wrap it into blob of (BatchWidth,) shape\n",
    "        return neoml.Blob.asblob(math_engine, data,\n",
    "                                 (1, shape[0], 1, 1, 1, 1, 1))\n",
    "    else:\n",
    "        assert(False)\n",
    "\n",
    "\n",
    "def cifar10_array_iter(X, y, batch_size):\n",
    "    \"\"\"Slices numpy arrays into batches\"\"\"\n",
    "    start = 0\n",
    "    data_size = y.shape[0]\n",
    "    while start < data_size:\n",
    "        yield X[start : start+batch_size], y[start : start+batch_size]\n",
    "        start += batch_size\n",
    "\n",
    "\n",
    "def cifar10_blob_iter(X, y, batch_size, math_engine):\n",
    "    \"\"\"Slices numpy arrays into batches and wraps them in blobs\"\"\"\n",
    "    for X_b, y_b in cifar10_array_iter(X, y, batch_size):\n",
    "        yield make_blob(X_b, math_engine), make_blob(y_b, math_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-latest",
   "metadata": {},
   "source": [
    "To train the network, call `dnn.learn` with data as its argument.\n",
    "\n",
    "To run the network without training, call `dnn.run` with data as its argument.\n",
    "\n",
    "The input data is a `dict` where each key is a `neoml.Dnn.Source` layer name and the corresponding value is the `neoml.Blob.Blob` that should be passed in to this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "academic-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net(X, y, batch_size, dnn, is_train):\n",
    "    \"\"\"Runs dnn on given data\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0.\n",
    "    run_iter = dnn.learn if is_train else dnn.run\n",
    "    math_engine = dnn.math_engine\n",
    "    layers = dnn.layers\n",
    "    loss = layers['loss']\n",
    "    accuracy = layers['accuracy']\n",
    "    sink = layers['accuracy_sink']\n",
    "\n",
    "    accuracy.reset = True  # Reset previous statistics\n",
    "    # Iterate over batches\n",
    "    for X_batch, y_batch in cifar10_blob_iter(X, y, batch_size, math_engine):\n",
    "        # Run the network on the batch data\n",
    "        run_iter({'data': X_batch, 'labels': y_batch})\n",
    "        total_loss += loss.last_loss * y_batch.batch_width  # Update epoch loss\n",
    "        accuracy.reset = False  # Don't reset statistics within one epoch\n",
    "\n",
    "    avg_loss = total_loss / y.shape[0]\n",
    "    avg_acc = sink.get_blob().asarray()[0]\n",
    "    run_time = time.time() - start\n",
    "    return avg_loss, avg_acc, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-spouse",
   "metadata": {},
   "source": [
    "In this tutorial, we'll also demonstrate how to store and load progress during training.\n",
    "\n",
    "Store training progress using the `dnn.store_checkpoint` method. Resume training from checkpoint by calling `dnn.load_checkpoint`.\n",
    "\n",
    "**Important:** NeoML checkpoints contain all the information required for training, *including the net architecture*. That allows us to `load_checkpoint` into any `neoml.Dnn.Dnn` object without the need to re-create architecture or solver before loading. However, this leads to the creation of new layer, solver, and blob objects during each `dnn.load_checkpoint`. If you had any previously created python variables which were pointing to the objects of the net *before loading* (like `solver`, `data` variables here), you must re-initialize them with the new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subsequent-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train #0\tLoss: 1.5371\tAccuracy: 0.4499\tTime: 6.12 sec\n",
      "Test  #0\tLoss: 1.2951\tAccuracy: 0.5341\tTime: 0.71 sec\n",
      "Train #1\tLoss: 1.2288\tAccuracy: 0.5630\tTime: 5.87 sec\n",
      "Test  #1\tLoss: 1.1361\tAccuracy: 0.5951\tTime: 0.71 sec\n",
      "Creating checkpoint...\n",
      "Train #2\tLoss: 1.1138\tAccuracy: 0.6064\tTime: 5.90 sec\n",
      "Test  #2\tLoss: 1.2091\tAccuracy: 0.5761\tTime: 0.70 sec\n",
      "Train #3\tLoss: 1.0385\tAccuracy: 0.6321\tTime: 5.91 sec\n",
      "Test  #3\tLoss: 1.0687\tAccuracy: 0.6177\tTime: 0.71 sec\n",
      "Train #4\tLoss: 0.9907\tAccuracy: 0.6520\tTime: 5.92 sec\n",
      "Test  #4\tLoss: 1.0566\tAccuracy: 0.6293\tTime: 0.70 sec\n",
      "Train #5\tLoss: 0.9494\tAccuracy: 0.6647\tTime: 5.93 sec\n",
      "Test  #5\tLoss: 1.0407\tAccuracy: 0.6361\tTime: 0.70 sec\n",
      "Loading checkpoint... (this will roll dnn back to epoch #1)\n",
      "Train #6\tLoss: 1.1142\tAccuracy: 0.6035\tTime: 5.91 sec\n",
      "Test  #6\tLoss: 1.0858\tAccuracy: 0.6211\tTime: 0.70 sec\n",
      "Train #7\tLoss: 1.0434\tAccuracy: 0.6333\tTime: 5.95 sec\n",
      "Test  #7\tLoss: 1.1010\tAccuracy: 0.6111\tTime: 0.71 sec\n",
      "Train #8\tLoss: 0.9904\tAccuracy: 0.6490\tTime: 5.89 sec\n",
      "Test  #8\tLoss: 1.0438\tAccuracy: 0.6331\tTime: 0.71 sec\n",
      "Train #9\tLoss: 0.9491\tAccuracy: 0.6673\tTime: 5.94 sec\n",
      "Test  #9\tLoss: 1.0083\tAccuracy: 0.6476\tTime: 0.71 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Network params\n",
    "batch_size = 50\n",
    "\n",
    "n_epoch = 10\n",
    "for epoch in range(n_epoch):\n",
    "    # Train\n",
    "    avg_loss, acc, run_time = run_net(X_train, y_train, batch_size,\n",
    "                                      dnn, is_train=True)\n",
    "    print(f'Train #{epoch}\\tLoss: {avg_loss:.4f}\\t'\n",
    "          f'Accuracy: {acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "    # Test\n",
    "    avg_loss, acc, run_time = run_net(X_test, y_test, batch_size,\n",
    "                                      dnn, is_train=False)\n",
    "    print(f'Test  #{epoch}\\tLoss: {avg_loss:.4f}\\t'\n",
    "          f'Accuracy: {acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "    if epoch == 1:\n",
    "        # If you want to save training progress you can do it via checkpoints\n",
    "        # that store dnn weights and other training data (solver stats, etc.)\n",
    "        print('Creating checkpoint...')\n",
    "        dnn.store_checkpoint('cifar10_sample.checkpoint')\n",
    "    if epoch == 5:\n",
    "        # Resume training from the checkpoint\n",
    "        print('Loading checkpoint... (this will roll dnn back to epoch #1)')\n",
    "        dnn.load_checkpoint('cifar10_sample.checkpoint')\n",
    "        # Be careful! dnn now points to the new net\n",
    "        # But other layer/solver variables still pointing to the old net!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-breed",
   "metadata": {},
   "source": [
    "## Prepare the network for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-civilization",
   "metadata": {},
   "source": [
    "Before using the trained network for inferenceâthat is, just to process data with no more changes to the network itselfâwe must delete training-only layers. All layers that receive correct labels as input should be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "narrow-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove training-only layers\n",
    "dnn.delete_layer('labels')\n",
    "dnn.delete_layer('loss')\n",
    "dnn.delete_layer('accuracy')\n",
    "dnn.delete_layer('accuracy_sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-manhattan",
   "metadata": {},
   "source": [
    "We still need a sink layer to extract logits. If you need *exact* normalized probabilities you should add `neoml.Dnn.Softmax` layer before sink. But we're only interested in the index of the most probable class, so we'll omit softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rough-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sink for dnn output\n",
    "sink = neoml.Dnn.Sink(dnn.layers['fc'], name='sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-eight",
   "metadata": {},
   "source": [
    "We'll also fuse each batch normalization layer with the previous convolution or fully-connected layer, to reduce the number of operations during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appointed-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_batch_norm(dnn, block_name):\n",
    "    \"\"\"Fuses batch_norm into convolution to reduce inference time\n",
    "    Should be used after training\n",
    "    \"\"\"\n",
    "    bn_name = block_name + '_bn'\n",
    "    if not dnn.has_layer(bn_name):\n",
    "        # Batch norm has already been fused\n",
    "        return\n",
    "    bn_layer = dnn.layers[bn_name]\n",
    "    conv_name = block_name + '_conv'\n",
    "    conv_layer = dnn.layers[conv_name]\n",
    "    # Fuse batch normalization\n",
    "    conv_layer.apply_batch_normalization(bn_layer)\n",
    "    # Delete layer from net (conv already 'contains' it)\n",
    "    dnn.delete_layer(bn_name)\n",
    "    # Connect layer after batchnorm to convolution\n",
    "    # because batchnorm was removed from the dnn\n",
    "    output_name = block_name + '_relu6'\n",
    "    dnn.layers[output_name].connect(conv_layer)\n",
    "\n",
    "\n",
    "# Fuse batchnorms into convolutions\n",
    "fuse_batch_norm(dnn, 'block1')\n",
    "fuse_batch_norm(dnn, 'block2')\n",
    "fuse_batch_norm(dnn, 'block2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-uruguay",
   "metadata": {},
   "source": [
    "## Serialize the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-brooklyn",
   "metadata": {},
   "source": [
    "Use `dnn.store` to save the trained network. This method stores all the info required for inference. Later you may load the saved network by calling `dnn.load` on any `neoml.Dnn.Dnn` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "damaged-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store trained network\n",
    "# store/load methods work best for this because, unlike checkpoints,\n",
    "# they don't save training-related data and take up less disk space\n",
    "dnn.store('cifar10_sample.dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "biological-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained network\n",
    "dnn.load('cifar10_sample.dnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-semester",
   "metadata": {},
   "source": [
    "Once again, `load` leads to creation of the new layer objects. The `sink` variable created before loading now points to the old sink layer object, which doesn't belong to the new network. Let's fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "terminal-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = dnn.layers['sink']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-extra",
   "metadata": {},
   "source": [
    "## Evaluate the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-iceland",
   "metadata": {},
   "source": [
    "We can get the blob with the results of the last `dnn.run` by calling `sink.get_blob`. Then we'll convert it into a numpy array via `blob.asarray` and calculate accuracy by the means of numpy.\n",
    "\n",
    "We expect to get the same value for accuracy as during the test of the last epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "other-worth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference net test accuracy: 0.6476\n"
     ]
    }
   ],
   "source": [
    "# Evaluate inference\n",
    "inference_acc = 0.\n",
    "for X_b, y_b in cifar10_array_iter(X_test, y_test, batch_size):\n",
    "    dnn.run({'data': make_blob(X_b, math_engine)})\n",
    "    # Extract data from sink\n",
    "    # Non-normalized probabilities of shape (batch_size, n_classes)\n",
    "    logits = sink.get_blob().asarray()\n",
    "    # Calculate accuracy\n",
    "    inference_acc += (np.argmax(logits, axis=1) == y_b).sum()\n",
    "inference_acc /= len(X_test)\n",
    "\n",
    "# This number is expected to equal the test accuracy of the last epoch\n",
    "print(f'Inference net test accuracy: {inference_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
