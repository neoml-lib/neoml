{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright Â© 2017-2021 ABBYY Production LLC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#@title\r\n",
    "# \r\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
    "# you may not use this file except in compliance with the License.\r\n",
    "# You may obtain a copy of the License at\r\n",
    "#\r\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
    "#\r\n",
    "# Unless required by applicable law or agreed to in writing, software\r\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
    "# See the License for the specific language governing permissions and\r\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Porting neural networks\r\n",
    "\r\n",
    "## Introduction\r\n",
    "In this tutorial, we'll demonstrate how to transfer pretrained neural network from Keras to NeoML.Dnn for further tuning and inference.\r\n",
    "\r\n",
    "The experimental setup consists of a model with embedding layer followed by a fully connected layer and a bidirectional LSTM. The aim of this toy example is to show the basic principles of migration, therefore we do not make any training and use small dimension lengths to make tensors human-readable. The considered model is unlikely to solve any real task.\r\n",
    "\r\n",
    "The tutorial includes the following steps:\r\n",
    "1. [Setup](#Setup)\r\n",
    "1. [Prepare models](#Prepare-models)\r\n",
    "1. [Weights transform and transfer](#Weights-transform-and-transfer)\r\n",
    "1. [Checking the results](#Check-the-results)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\r\n",
    "\r\n",
    "To prevent possible GPU memory shortage, force Keras to use CPU. This step is optional."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import typing as tp\r\n",
    "import numpy as np \r\n",
    "np.random.RandomState(42)\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "\r\n",
    "import neoml"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare models\r\n",
    "\r\n",
    "First train your Keras model to get the weigths to transfer (here we transfer random-initialized model). Then you need a NeoML model with the equivalent architecture. Some versions of TensorFlow and NeoML are incompatible due to different CUDA or numpy requirements, fortunately, simultaneous launch is convenient but not mandatory. In this case, create two environments, train Keras and save weigths to files (ex. *numpy.save*), then load weights and launch NeoML in the other environment.\r\n",
    "\r\n",
    "Compliance table for layers used in this tutorial:\r\n",
    "\r\n",
    "Keras | NeoML\r\n",
    "--- | ---\r\n",
    "Input | Source\r\n",
    "Embedding | MultichannelLookup\r\n",
    "Dense | FullyConnected\r\n",
    "Dense with activation | FullyConnected + layer from Dnn.Activation\r\n",
    "LSTM | LSTM\r\n",
    "Bidirectional( *layer* ) | 2 x *layer* + Concat\r\n",
    "*output* | Sink\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "VOCAB_SIZE = 5\r\n",
    "EMBEDDING_DIM = 5\r\n",
    "DENSE_DIM = 2\r\n",
    "LSTM_DIM = 3\r\n",
    "NUM_CLASSES = 2\r\n",
    "BATCH_SIZE = 3\r\n",
    "SEQ_LEN = 2\r\n",
    "\r\n",
    "SOURCE_LAYER_NAME = \"Input\"\r\n",
    "EMBEDDING_LAYER_NAME = \"Embedding\"\r\n",
    "DENSE_LAYER_NAME = \"Dense\"\r\n",
    "LSTM_LAYER_NAME = \"LSTM\"\r\n",
    "FWD_LSTM_LAYER_NAME = \"LSTM-forward\"\r\n",
    "BWD_LSTM_LAYER_NAME = \"LSTM-backward\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def build_keras_model() -> keras.Model:\r\n",
    "    inputs = keras.layers.Input(shape=(None,), dtype=np.int32)\r\n",
    "\r\n",
    "    embeddings = keras.layers.Embedding(name=EMBEDDING_LAYER_NAME,\r\n",
    "                                        output_dim=EMBEDDING_DIM,\r\n",
    "                                        input_dim=VOCAB_SIZE)(inputs)\r\n",
    "\r\n",
    "    dense = keras.layers.Dense(name=DENSE_LAYER_NAME, units=DENSE_DIM, activation='tanh')(embeddings)\r\n",
    "    \r\n",
    "    lstm = keras.layers.Bidirectional(name=LSTM_LAYER_NAME,\r\n",
    "                                      layer=keras.layers.LSTM(\r\n",
    "                                          units=LSTM_DIM,\r\n",
    "                                          return_sequences=True,\r\n",
    "                                          return_state=True))(dense)\r\n",
    "    outputs = [dense, lstm]\r\n",
    "    return keras.Model(inputs, outputs)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class NeomlModel():\r\n",
    "    def __init__(self, useCuda=False):\r\n",
    "        self._engine = neoml.MathEngine.GpuMathEngine(0) if useCuda else neoml.MathEngine.CpuMathEngine(0)\r\n",
    "        self._dnn = neoml.Dnn.Dnn(self._engine)\r\n",
    "        source = neoml.Dnn.Source(self._dnn, SOURCE_LAYER_NAME)\r\n",
    "\r\n",
    "        embedding = neoml.Dnn.MultichannelLookup(\r\n",
    "            source, dimensions=[(VOCAB_SIZE, EMBEDDING_DIM)], name=EMBEDDING_LAYER_NAME)\r\n",
    "        neoml.Dnn.Sink(embedding, name=EMBEDDING_LAYER_NAME + \"-output\")\r\n",
    "\r\n",
    "        dense = neoml.Dnn.FullyConnected(embedding, DENSE_DIM, name=\"Dense\")\r\n",
    "        dense_activated = neoml.Dnn.Activation.Tanh(dense)\r\n",
    "        neoml.Dnn.Sink(dense_activated, name=DENSE_LAYER_NAME + \"-output\")\r\n",
    "\r\n",
    "        fwd_rnn = neoml.Dnn.Lstm(dense_activated, LSTM_DIM, name=FWD_LSTM_LAYER_NAME)\r\n",
    "        bwd_rnn = neoml.Dnn.Lstm(dense_activated, LSTM_DIM, name=BWD_LSTM_LAYER_NAME, reverse_seq=True)\r\n",
    "        join_rnn = neoml.Dnn.Concat.ConcatChannels(input_layers=[fwd_rnn, bwd_rnn])\r\n",
    "        # You don't need to insert so many sink layers normally. We create them for debug purposes only.\r\n",
    "        neoml.Dnn.Sink(join_rnn, name=LSTM_LAYER_NAME + \"-output\")\r\n",
    "        s = neoml.Dnn.Sink(fwd_rnn, name=FWD_LSTM_LAYER_NAME + \"-cell\")\r\n",
    "        s.connect(fwd_rnn, 1)\r\n",
    "        s = neoml.Dnn.Sink(bwd_rnn, name=BWD_LSTM_LAYER_NAME + \"-cell\")\r\n",
    "        s.connect(bwd_rnn, 1)\r\n",
    "\r\n",
    "    @property\r\n",
    "    def engine(self) -> neoml.MathEngine:\r\n",
    "        return self._engine\r\n",
    "\r\n",
    "    @property\r\n",
    "    def dnn(self) -> neoml.Dnn:\r\n",
    "        return self._dnn\r\n",
    "\r\n",
    "    def __getitem__(self, key: str) -> neoml.Dnn.Layer:\r\n",
    "        return self._dnn.layers[key]\r\n",
    "\r\n",
    "    def asblob(self, array: np.ndarray, shape: tp.Tuple[7 * (float, )]) -> neoml.Blob:\r\n",
    "        assert array.size == np.prod(shape), \"check array size\"\r\n",
    "        assert array.shape == tuple(i for i in shape if i != 1), \"check the order of dimensions, transpose if needed\"\r\n",
    "        return neoml.Blob.asblob(self._engine, array, shape)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "keras_model = build_keras_model()\r\n",
    "neo_model = NeomlModel()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# create same pieces of data to feed the models\r\n",
    "np_blob = np.random.randint(1, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN))\r\n",
    "neo_blob = neo_model.asblob(np_blob.T, (SEQ_LEN, BATCH_SIZE, 1, 1, 1, 1, 1))\r\n",
    "\r\n",
    "# just to initialize NEOmodel\r\n",
    "neo_model.dnn.run({SOURCE_LAYER_NAME: neo_blob})\r\n",
    "np_blob"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 3],\n",
       "       [4, 4]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weights transform and transfer\r\n",
    "Finally we can load the prepared weights into the NeoML model. As you will see below, most weights can be transferred 'as-is'. The only exception is LSTM. Note the main differences between Keras and NeoML implementations:\r\n",
    "- Keras LSTM is batch-first by default while NeoML LSTM is time-first. In terms of the Seven-dimensional tensor, *batch length* is a sequence length, *batch width* is a batch size, and *channels* is a hidden dimension length.\r\n",
    "- NeoML doesn't support [Keras masks](https://keras.io/guides/understanding_masking_and_padding/): LSTM processes the whole sequence with padding.\r\n",
    "- The order of gates in concatenated kernels is non-standard: (cell | forget | input | output) instead of (input | forget | cell | output). In addition, *cell* is called *main gate*, *output gate* -- *reset gate*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embedding layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "embedding_blob = neo_model.asblob(keras_model.get_layer(EMBEDDING_LAYER_NAME).weights[0].numpy(),\r\n",
    "                                  neo_model[EMBEDDING_LAYER_NAME].get_embeddings(0).shape)\r\n",
    "neo_model[EMBEDDING_LAYER_NAME].set_embeddings(index=0, blob=embedding_blob)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dense layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "keras_dense = keras_model.get_layer(DENSE_LAYER_NAME)\r\n",
    "dense_kernel = neo_model.asblob(keras_dense.weights[0].numpy().T, neo_model[DENSE_LAYER_NAME].weights.shape)\r\n",
    "dense_free_term = neo_model.asblob(keras_dense.weights[1].numpy(), neo_model[DENSE_LAYER_NAME].free_term.shape)\r\n",
    "neo_model[DENSE_LAYER_NAME].weights = dense_kernel\r\n",
    "neo_model[DENSE_LAYER_NAME].free_term = dense_free_term"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def reorder_lstm(weights: np.ndarray, lstm_dim: int) -> np.ndarray:\r\n",
    "    permutation = [2, 1, 0, 3]\r\n",
    "    # free_term\r\n",
    "    if weights.numpy().size == 4 * lstm_dim:\r\n",
    "        return weights.numpy().T.reshape(4, lstm_dim)[permutation].reshape(4 * lstm_dim) \r\n",
    "    # matrix\r\n",
    "    else:\r\n",
    "        return weights.numpy().T.reshape(4, lstm_dim, -1)[permutation].reshape(4 * lstm_dim, -1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "keras_lstm = keras_model.get_layer(LSTM_LAYER_NAME)\r\n",
    "\r\n",
    "forward_input_kernel = neo_model.asblob(reorder_lstm(keras_lstm.weights[0], LSTM_DIM), neo_model[FWD_LSTM_LAYER_NAME].input_weights.shape)\r\n",
    "forward_rec_kernel = neo_model.asblob(reorder_lstm(keras_lstm.weights[1], LSTM_DIM), neo_model[FWD_LSTM_LAYER_NAME].recurrent_weights.shape)\r\n",
    "forward_free_term = neo_model.asblob(reorder_lstm(keras_lstm.weights[2], LSTM_DIM), neo_model[FWD_LSTM_LAYER_NAME].input_free_term.shape)\r\n",
    "backward_input_kernel = neo_model.asblob(reorder_lstm(keras_lstm.weights[3], LSTM_DIM), neo_model[BWD_LSTM_LAYER_NAME].input_weights.shape)\r\n",
    "backward_rec_kernel = neo_model.asblob(reorder_lstm(keras_lstm.weights[4], LSTM_DIM), neo_model[BWD_LSTM_LAYER_NAME].recurrent_weights.shape)\r\n",
    "backward_free_term = neo_model.asblob(reorder_lstm(keras_lstm.weights[5], LSTM_DIM), neo_model[BWD_LSTM_LAYER_NAME].input_free_term.shape)\r\n",
    "# NeoML's LSTM has two free terms to be compatible with ONNX and PyTorch, we leave one of them filled with zeros\r\n",
    "zero_free_term = neo_model.asblob(np.zeros(4 * LSTM_DIM, dtype=np.float32), neo_model[FWD_LSTM_LAYER_NAME].recurrent_free_term.shape)\r\n",
    "\r\n",
    "neo_model[FWD_LSTM_LAYER_NAME].input_weights = forward_input_kernel\r\n",
    "neo_model[FWD_LSTM_LAYER_NAME].recurrent_weights = forward_rec_kernel\r\n",
    "neo_model[FWD_LSTM_LAYER_NAME].input_free_term = forward_free_term\r\n",
    "neo_model[FWD_LSTM_LAYER_NAME].recurrent_free_term = zero_free_term\r\n",
    "neo_model[BWD_LSTM_LAYER_NAME].input_weights = backward_input_kernel\r\n",
    "neo_model[BWD_LSTM_LAYER_NAME].recurrent_weights = backward_rec_kernel\r\n",
    "neo_model[BWD_LSTM_LAYER_NAME].input_free_term = backward_free_term\r\n",
    "neo_model[BWD_LSTM_LAYER_NAME].recurrent_free_term = zero_free_term\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "keras_dense_out, keras_lstm_outs = keras_model(np_blob)\r\n",
    "keras_lstm_out = keras_lstm_outs[0].numpy()\r\n",
    "# No need to check h_n since the LSTM has single layer and all hidden states are included in lstm_out\r\n",
    "keras_fwd_cell = keras_lstm_outs[2].numpy()\r\n",
    "keras_bwd_cell = keras_lstm_outs[4].numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "output = neo_model.dnn.run({\"Input\": neo_blob})\r\n",
    "neo_dense_out = output[DENSE_LAYER_NAME + \"-output\"].asarray().transpose((1,0,2))\r\n",
    "neo_lstm_out = output[LSTM_LAYER_NAME + \"-output\"].asarray().transpose((1,0,2))\r\n",
    "# NeoML returns all cell states for each timestep\r\n",
    "neo_fwd_cell = output[FWD_LSTM_LAYER_NAME + \"-cell\"].asarray()[-1]\r\n",
    "neo_bwd_cell = output[BWD_LSTM_LAYER_NAME + \"-cell\"].asarray()[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "(\r\n",
    "    abs(neo_dense_out - keras_dense_out.numpy()).max(),\r\n",
    "    abs(neo_lstm_out - keras_lstm_out).max(),\r\n",
    "    abs(neo_fwd_cell - keras_fwd_cell).max(),\r\n",
    "    abs(neo_bwd_cell - keras_bwd_cell).max()\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9.313226e-08, 6.030314e-08, 5.401671e-08, 9.49949e-08)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('ind': conda)"
  },
  "interpreter": {
   "hash": "6af946c9753a5dee58a1cc12ca7a224441f63cb0bd30ef1cb60ab388dbdb9aef"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}