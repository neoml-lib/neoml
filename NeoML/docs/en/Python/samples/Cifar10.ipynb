{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "disabled-singing",
   "metadata": {},
   "source": [
    "Copyright Â© 2017-2021 ABBYY Production LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interstate-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-bahrain",
   "metadata": {},
   "source": [
    "# Cifar-10 neural net sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-banking",
   "metadata": {},
   "source": [
    "This example make the following:\n",
    "\n",
    "* Downloads and prepares CIFAR-10 dataset\n",
    "* Builds the net for training\n",
    "* Trains the net on the dataset\n",
    "* Prepares the net for inference\n",
    "* Evaluates the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neoml\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tarfile\n",
    "\n",
    "np.random.seed(666)\n",
    "\n",
    "\n",
    "def calc_md5(file_name):\n",
    "    \"\"\"Calculates md5 hash of an existing file\"\"\"\n",
    "    import hashlib\n",
    "    curr_hash = hashlib.md5()\n",
    "    with open(file_name, 'rb') as file_in:\n",
    "        chunk = file_in.read(8192)\n",
    "        while chunk:\n",
    "            curr_hash.update(chunk)\n",
    "            chunk = file_in.read(8192)\n",
    "    return curr_hash.hexdigest()\n",
    "\n",
    "\n",
    "# Download data\n",
    "url = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "file_name = url[url.rfind('/')+1:]\n",
    "ARCHIVE_SIZE = 170498071\n",
    "ARCHIVE_MD5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "\n",
    "# Download when archive is missing or broken\n",
    "if (not os.path.isfile(file_name)) \\\n",
    "        or os.path.getsize(file_name) != ARCHIVE_SIZE \\\n",
    "        or calc_md5(file_name) != ARCHIVE_MD5:\n",
    "    import requests\n",
    "    with requests.get(url, stream=True) as url_stream:\n",
    "        url_stream.raise_for_status()\n",
    "        with open(file_name, 'wb') as file_out:\n",
    "            for chunk in url_stream.iter_content(chunk_size=8192):\n",
    "                file_out.write(chunk)\n",
    "\n",
    "# Unpack data\n",
    "tar = tarfile.open(file_name, 'r:gz')\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "\n",
    "def load_batch(file_name):\n",
    "    \"\"\"Loads data from one of the batch files\"\"\"\n",
    "    import pickle\n",
    "    with open(file_name, 'rb') as file_in:\n",
    "        result = pickle.load(file_in, encoding='bytes')\n",
    "    return result\n",
    "\n",
    "\n",
    "def transform_data(X):\n",
    "    \"\"\"Normalizes and transposes data for NeoML\"\"\"\n",
    "    X = X.astype(np.float32)\n",
    "    X = (X - 127.5) / 255.\n",
    "    X = X.reshape((X.shape[0], 3, 32, 32))\n",
    "    X = X.transpose((0, 2, 3, 1))  # NeoML uses channel-last pack\n",
    "    return X\n",
    "\n",
    "\n",
    "# Preparing data\n",
    "batch_name = 'cifar-10-batches-py/data_batch_{0}'\n",
    "train_data = [load_batch(batch_name.format(i)) for i in range(1, 6)]\n",
    "X_train = np.concatenate(list(x[b'data'] for x in train_data), axis=0)\n",
    "X_train = transform_data(X_train)\n",
    "y_train = np.concatenate(list(x[b'labels'] for x in train_data), axis=0)\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "test_data = load_batch('cifar-10-batches-py/test_batch')\n",
    "X_test = test_data[b'data']\n",
    "X_test = transform_data(X_test)\n",
    "y_test = np.array(test_data[b'labels'], dtype=np.int32)\n",
    "\n",
    "\n",
    "def make_blob(data, math_engine):\n",
    "    \"\"\"Wraps numpy data into neoml blob\"\"\"\n",
    "    shape = data.shape\n",
    "    if len(shape) == 4:  # data\n",
    "        # Wrap 4-D array into (BatchWidth, Height, Width, Channels) blob\n",
    "        blob_shape = (1, shape[0], 1, shape[1], shape[2], 1, shape[3])\n",
    "        return neoml.Blob.asblob(math_engine, data, blob_shape)\n",
    "    elif len(shape) == 1:  # dense labels\n",
    "        # Wrap 1-D array into blob of (BatchWidth,) shape\n",
    "        return neoml.Blob.asblob(math_engine, data,\n",
    "                                 (1, shape[0], 1, 1, 1, 1, 1))\n",
    "    else:\n",
    "        assert(False)\n",
    "\n",
    "\n",
    "def cifar10_array_iter(X, y, batch_size):\n",
    "    \"\"\"Slices numpy arrays into batches\"\"\"\n",
    "    start = 0\n",
    "    data_size = y.shape[0]\n",
    "    while start < data_size:\n",
    "        yield X[start : start+batch_size], y[start : start+batch_size]\n",
    "        start += batch_size\n",
    "\n",
    "\n",
    "def cifar10_blob_iter(X, y, batch_size, math_engine):\n",
    "    \"\"\"Slices numpy arrays into batches and wraps them in blobs\"\"\"\n",
    "    for X_b, y_b in cifar10_array_iter(X, y, batch_size):\n",
    "        yield make_blob(X_b, math_engine), make_blob(y_b, math_engine)\n",
    "\n",
    "\n",
    "def run_net(X, y, batch_size, dnn, is_train):\n",
    "    \"\"\"Runs dnn on given data\"\"\"\n",
    "    start = time.time()\n",
    "    total_loss = 0.\n",
    "    run_iter = dnn.learn if is_train else dnn.run\n",
    "    math_engine = dnn.math_engine\n",
    "    layers = dnn.layers\n",
    "    loss = layers['loss']\n",
    "    accuracy = layers['accuracy']\n",
    "    sink = layers['accuracy_sink']\n",
    "\n",
    "    accuracy.reset = True  # Reset previous statistics\n",
    "    for X_batch, y_batch in cifar10_blob_iter(X, y, batch_size, math_engine):\n",
    "        run_iter({'data': X_batch, 'labels': y_batch})\n",
    "        total_loss += loss.last_loss * y_batch.batch_width\n",
    "        accuracy.reset = False  # Don't reset statistics within one epoch\n",
    "\n",
    "    avg_loss = total_loss / y.shape[0]\n",
    "    avg_acc = sink.get_blob().asarray()[0]\n",
    "    run_time = time.time() - start\n",
    "    return avg_loss, avg_acc, run_time\n",
    "\n",
    "\n",
    "class ConvBlock:\n",
    "    \"\"\"Block of dropout->conv->batch_norm->relu6\"\"\"\n",
    "    def __init__(self, inputs, filter_count, name):\n",
    "        self.dropout = neoml.Dnn.Dropout(inputs, rate=0.1, spatial=True,\n",
    "                                         batchwise=True, name=name+'_dropout')\n",
    "        self.conv = neoml.Dnn.Conv(self.dropout, filter_count=filter_count,\n",
    "                                   filter_size=(3, 3), stride_size=(2, 2),\n",
    "                                   padding_size=(1, 1), name=name+'_conv')\n",
    "        self.bn = neoml.Dnn.BatchNormalization(self.conv, channel_based=True,\n",
    "                                               name=name+'_bn')\n",
    "        self.output = neoml.Dnn.ReLU(self.bn, threshold=6., name=name+'_relu6')\n",
    "\n",
    "\n",
    "# Create math engine\n",
    "math_engine = neoml.MathEngine.GpuMathEngine(0)\n",
    "# If GPU can't be found it will return CPU math engine\n",
    "print('Device: ', math_engine.info)\n",
    "\n",
    "# Create net\n",
    "dnn = neoml.Dnn.Dnn(math_engine)\n",
    "\n",
    "# Network params\n",
    "batch_size = 50\n",
    "lr = 1e-3\n",
    "n_classes = 10\n",
    "\n",
    "# Create layers\n",
    "data = neoml.Dnn.Source(dnn, 'data')  # Source for data\n",
    "labels = neoml.Dnn.Source(dnn, 'labels')  # Source for labels\n",
    "# Add a few convolutional blocks\n",
    "block1 = ConvBlock(data, filter_count=16, name='block1')  # -> (16,  16)\n",
    "block2 = ConvBlock(block1.output, filter_count=32, name='block2')  # -> (8, 8)\n",
    "block3 = ConvBlock(block2.output, filter_count=64, name='block3')  # -> (4, 4)\n",
    "# Fully connected flattens its input automatically\n",
    "fc = neoml.Dnn.FullyConnected(block3.output, n_classes, name='fc')\n",
    "# Softmax is applied within cross-entropy\n",
    "loss = neoml.Dnn.CrossEntropyLoss((fc, labels), name='loss')\n",
    "# Auxilary layers in order to get statistics\n",
    "accuracy = neoml.Dnn.Accuracy((fc, labels), name='accuracy')\n",
    "accuracy_sink = neoml.Dnn.Sink(accuracy, name='accuracy_sink')\n",
    "\n",
    "# Create solver\n",
    "dnn.solver = neoml.Dnn.AdaptiveGradient(math_engine, learning_rate=lr,\n",
    "                                        l1=0., l2=0.,  # No regularization\n",
    "                                        max_gradient_norm=1.,  # clip grad\n",
    "                                        moment_decay_rate=0.9,\n",
    "                                        second_moment_decay_rate=0.999)\n",
    "\n",
    "n_epoch = 10\n",
    "for epoch in range(n_epoch):\n",
    "    # Train\n",
    "    avg_loss, acc, run_time = run_net(X_train, y_train, batch_size,\n",
    "                                      dnn, is_train=True)\n",
    "    print(f'Train #{epoch}\\tLoss: {avg_loss:.4f}\\t'\n",
    "          f'Accuracy: {acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "    # Test\n",
    "    avg_loss, acc, run_time = run_net(X_test, y_test, batch_size,\n",
    "                                      dnn, is_train=False)\n",
    "    print(f'Test  #{epoch}\\tLoss: {avg_loss:.4f}\\t'\n",
    "          f'Accuracy: {acc:.4f}\\tTime: {run_time:.2f} sec')\n",
    "    if epoch == 1:\n",
    "        # If you want to save training progress you can do it via checkpoints\n",
    "        # It stores dnn weights and other training data (solver stats, etc.)\n",
    "        print('Creating checkpoint...')\n",
    "        dnn.store_checkpoint('cifar10_sample.checkpoint')\n",
    "    if epoch == 5:\n",
    "        # If you want you can resume training from the checkpoint\n",
    "        print('Loading checkpoint... (this will roll dnn back to epoch #1)')\n",
    "        dnn.load_checkpoint('cifar10_sample.checkpoint')\n",
    "        # Be careful! dnn now points to the new net\n",
    "        # But other layer/solver variables still pointing to the old net!\n",
    "\n",
    "# Prepare network for inference\n",
    "\n",
    "# Remove training-only layers\n",
    "dnn.delete_layer('labels')\n",
    "dnn.delete_layer('loss')\n",
    "dnn.delete_layer('accuracy')\n",
    "dnn.delete_layer('accuracy_sink')\n",
    "\n",
    "# Add sink for dnn output\n",
    "sink = neoml.Dnn.Sink(dnn.layers['fc'], name='sink')\n",
    "\n",
    "\n",
    "def fuse_batch_norm(dnn, block_name):\n",
    "    \"\"\"Fuses batch_norm into convolution\n",
    "    As a result reduces inference time\n",
    "    Should be used after training\n",
    "    \"\"\"\n",
    "    bn_name = block_name + '_bn'\n",
    "    if not dnn.has_layer(bn_name):\n",
    "        # Batch norm has already been fused\n",
    "        return\n",
    "    bn_layer = dnn.layers[bn_name]\n",
    "    conv_name = block_name + '_conv'\n",
    "    conv_layer = dnn.layers[conv_name]\n",
    "    # Fuse batch normalization\n",
    "    conv_layer.apply_batch_normalization(bn_layer)\n",
    "    # Delete layer from net (conv already 'contains' it)\n",
    "    dnn.delete_layer(bn_name)\n",
    "    # Connect layer after batchnorm to convolution\n",
    "    # because batchnorm was removed from the dnn\n",
    "    output_name = block_name + '_relu6'\n",
    "    dnn.layers[output_name].connect(conv_layer)\n",
    "\n",
    "\n",
    "# Fuse batchnorms into convolutions\n",
    "fuse_batch_norm(dnn, 'block1')\n",
    "fuse_batch_norm(dnn, 'block2')\n",
    "fuse_batch_norm(dnn, 'block2')\n",
    "\n",
    "# Store trained net\n",
    "# In that case it's better to use method load/store\n",
    "# Unlike checkpoints those aren't working with training-related data\n",
    "# As a result they use less disk space\n",
    "dnn.store('cifar10_sample.dnn')\n",
    "\n",
    "# Load trained net\n",
    "# It's done for sample purpose only\n",
    "# You may comment the next line and everything will be just fine\n",
    "dnn.load('cifar10_sample.dnn')\n",
    "\n",
    "# Be careful! Layer variables must be updated\n",
    "# because they're pointing to the layers of the old dnn\n",
    "sink = dnn.layers['sink']\n",
    "\n",
    "# Evaluate inference\n",
    "inference_acc = 0.\n",
    "for X_b, y_b in cifar10_array_iter(X_test, y_test, batch_size):\n",
    "    dnn.run({'data': make_blob(X_b, math_engine)})\n",
    "    # Extract data from sink\n",
    "    # unnormalized probs of shape (batch_size, n_classes)\n",
    "    logits = sink.get_blob().asarray()\n",
    "    # Calculate accuracy\n",
    "    inference_acc += (np.argmax(logits, axis=1) == y_b).sum()\n",
    "inference_acc /= len(X_test)\n",
    "\n",
    "# This number must be equal to the test accuracy of the last epoch\n",
    "print(f'Inference net test accuracy: {inference_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
