/* Copyright Â© 2017-2023 ABBYY

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--------------------------------------------------------------------------------------------------------------*/

#include <common.h>
#pragma hdrstop

#include <NeoOnnx/NeoOnnxImport.h>
#include <NeoML/Dnn/Layers/Onnx/OnnxTransformHelper.h>
#include <NeoML/Dnn/Layers/Onnx/OnnxTransposeHelper.h>
#include "Optimization/LayerNormFusionOptimizer.h"

namespace NeoOnnx {

namespace optimization {

static constexpr const char* const fusionNamePrefix{ "NormFusion_" };

// Information about layout change in ONNX
struct CLayerNormFusionOptimizer::CLayoutChange {
	CTensorLayout From; // Layout before this change
	CTensorLayout To; // Layout after this change

	bool operator==( const CLayoutChange& other ) const
	{ return From == other.From && To == other.To; }
	// Checks that this layout switches back layouts from other change
	bool IsInversionOf( const CLayoutChange& other ) const
	{ return From == other.To && To == other.From; }
};

// Selects the following chain of ONNX layers
//     -> Transform -> [Transpose ->]*
// connected to inputIndex'th input of inputLayer
// Chains like these are generated by NeoOnnx in order to change tensor layout
// If succeeded returns output connected to Transform's input and fills information about layouts before/after
// If failed returns empty output: CLayerOutput( nullptr, NotFound )
CLayerOutput<> CLayerNormFusionOptimizer::selectLayoutChange( CBaseLayer& inputLayer, int inputIndex,
	CLayoutChange& change ) const
{
	CBaseLayer* currentLayer = graph.SelectConnectedOutput<>( inputLayer, inputIndex, true ).Layer;
	CFastArray<COnnxTransposeHelper*, 8> transposes;

	while( dynamic_cast<COnnxTransformHelper*>( currentLayer ) == nullptr ) {
		transposes.Add( dynamic_cast<COnnxTransposeHelper*>( currentLayer ) );
		if( transposes.Last() == nullptr ) {
			return CLayerOutput<>(); // Unknown layer inside of layout change
		}
		currentLayer = graph.SelectTheOnlyConnectedOutput<>( *currentLayer, true );
	}

	COnnxTransformHelper& transform = dynamic_cast<COnnxTransformHelper&>( *currentLayer );
	transform.OutputLayout().CopyTo( change.To );
	change.From.Empty();
	for( int i = 0; i < change.To.Size(); ++i ) {
		change.From.Add( transform.GetRule( change.To[i] ) );
		NeoAssert( change.From.Last() != BD_Count );
	}

	for( int i = transposes.Size() - 1; i >= 0; --i ) {
		NeoAssert( transposes[i] != nullptr );
		TBlobDim firstDim = BD_Count;
		TBlobDim secondDim = BD_Count;
		transposes[i]->GetDims( firstDim, secondDim );
		const int firstIndex = change.To.Find( firstDim );
		const int secondIndex = change.To.Find( secondDim );
		NeoAssert( firstIndex != NotFound && secondIndex != NotFound && firstIndex != secondIndex );
		std::swap( change.To[firstIndex], change.To[secondIndex] );
	}

	return graph.GetConnectedOutput( transform, 0 );
}

// Checks if DataLayer is valid for CLayerNormFusionOptimizer conversion
bool CLayerNormFusionOptimizer::isValidDataLayer( const CDataLayer& dataLayer, TBlobType blobType, int blobSize ) const
{
	NeoAssert( graph.GetInputCount( dataLayer ) == 0 );
	NeoAssert( graph.GetOutputCount( dataLayer ) == 1 );

	if( graph.GetConnectedInputsCount( dataLayer, /*outputIndex*/0 ) != 1 ) {
		return false;
	}

	CPtr<CDnnBlob> blob = dataLayer.GetBlob();
	return ( blob->GetDataType() == blobType
		&& ( blobSize == NotFound || blob->GetDataSize() == blobSize ) );
}

// Checks if CastLayer is valid for CLayerNormFusionOptimizer conversion
bool CLayerNormFusionOptimizer::isValidCastLayer( const CCastLayer& castLayer ) const
{
	NeoAssert( graph.GetInputCount( castLayer ) == 1 );
	NeoAssert( graph.GetOutputCount( castLayer ) == 1 );

	return castLayer.GetOutputType() == CT_Float;
}

void CLayerNormFusionOptimizer::Apply()
{
	NeoAssert( graph.SelectionSize() == 0 );

	CArray<CBaseLayer*> layers{};
	graph.GetLayers( layers );
	for( auto& layer : layers ) {
		graph.ClearSelection();

		if( !graph.HasLayer( layer ) ) { // Skip already replaced layers
			continue;
		}

		// Searching for a group of layers to replace by an object normalization layer in the backward direction through the graph
		// From bottom to upside for graph
		auto* addLayerLast = dynamic_cast<COnnxEltwiseLayer*>( layer );
		if( addLayerLast == nullptr
			|| !isValidArithmeticLayer( *addLayerLast, COnnxEltwiseLayer::TOperation::Add )
			|| graph.IsLayerSelected( *addLayerLast ) )
		{
			continue; // fail this Fusion
		}
		graph.SelectLayer( *addLayerLast );

		CLayerOutput<CDataLayer> bias{};
		CLayerOutput<COnnxEltwiseLayer> mul{};
		if( !graph.SelectBothConnectedOutputs<COnnxEltwiseLayer, CDataLayer>( *addLayerLast, mul, bias, /*checkOutOfSelectionLinks*/false )
			|| !isValidArithmeticLayer( *mul.Layer, COnnxEltwiseLayer::TOperation::Mul )
			|| !isValidDataLayer( *bias.Layer, CT_Float, /*size*/NotFound ) )
		{
			continue; // fail this Fusion
		}

		CLayerOutput<CDataLayer> scale{};
		CLayerOutput<COnnxEltwiseLayer> div{};
		CLayerOutput<CCastLayer> uselessCast{}; // try to skip CAST layer as operand (1)
		if( !graph.SelectBothConnectedOutputs<CCastLayer, CDataLayer>( *mul.Layer, uselessCast, scale, /*checkOutOfSelectionLinks*/false )
			|| !isValidCastLayer( *uselessCast.Layer )
			|| !isValidDataLayer( *scale.Layer, CT_Float, /*size*/NotFound ) )
		{
			scale.Clear();
			uselessCast.Clear(); // try to skip CAST layer as operand (2)
			if( !graph.SelectBothConnectedOutputs<COnnxEltwiseLayer, CCastLayer>( *mul.Layer, div, uselessCast, /*checkOutOfSelectionLinks*/false )
				|| !isValidArithmeticLayer( *div.Layer, COnnxEltwiseLayer::TOperation::Div )
				|| !isValidCastLayer( *uselessCast.Layer ) )
			{
				div.Clear();
				uselessCast.Clear(); // no CAST layer as both of operands (3)
				if( !graph.SelectBothConnectedOutputs<COnnxEltwiseLayer, CDataLayer>( *mul.Layer, div, scale, /*checkOutOfSelectionLinks*/false )
					|| !isValidArithmeticLayer( *div.Layer, COnnxEltwiseLayer::TOperation::Div )
					|| !isValidDataLayer( *scale.Layer, CT_Float, /*size*/NotFound ) )
				{
					continue; // fail this Fusion
				}
			} else { // success to find the CAST layer as operand (2)
				scale.Layer = graph.SelectTheOnlyConnectedOutput<CDataLayer>( *uselessCast.Layer, /*checkOutOfSelectionLinks*/false );
				if( scale.Layer == nullptr
					|| !isValidDataLayer( *scale.Layer, CT_Float, /*size*/NotFound ) )
				{
					continue; // fail this Fusion
				}
			}
		} else { // success to find the CAST layer as operand (1)
			div.Layer = graph.SelectTheOnlyConnectedOutput<COnnxEltwiseLayer>( *uselessCast.Layer, /*checkOutOfSelectionLinks*/false );
			if( div.Layer == nullptr
				|| !isValidArithmeticLayer( *div.Layer, COnnxEltwiseLayer::TOperation::Div ) )
			{
				continue; // fail this Fusion
			}
		}

		CLayerOutput<> sqrtOutput{};
		CLayerOutput<COnnxEltwiseLayer> sub2{};
		CLayoutChange layoutChangeAfterPooling;
		for( int subIndex = 0; subIndex < 2; ++subIndex ) {
			sub2 = graph.SelectConnectedOutput<COnnxEltwiseLayer>( *div.Layer, subIndex, false );
			if( sub2.Layer != nullptr ) {
				sqrtOutput = selectLayoutChange( *div.Layer, 1 - subIndex, layoutChangeAfterPooling );
				break;
			}
		}

		auto* sqrtLayer = dynamic_cast<CPowerLayer*>( sqrtOutput.Layer );
		if( sqrtLayer == nullptr || !isValidPowerLayer( *sqrtLayer, 0.5f ) || sub2.Layer == nullptr
			|| !isValidArithmeticLayer( *sub2.Layer, COnnxEltwiseLayer::TOperation::Sub ) )
		{
			continue; // fail this Fusion
		}
		graph.SelectLayer( *sqrtLayer );

		COnnxEltwiseLayer* addLayer = graph.SelectTheOnlyConnectedOutput<COnnxEltwiseLayer>( *sqrtLayer, false );
		if( addLayer == nullptr || !isValidArithmeticLayer( *addLayer, COnnxEltwiseLayer::TOperation::Add ) ) {
			continue; // fail this Fusion
		}

		CLayerOutput<CGlobalMeanPoolingLayer> reduceMean2{};
		CLayerOutput<CDataLayer> eps{};
		if( !graph.SelectBothConnectedOutputs<CGlobalMeanPoolingLayer, CDataLayer>( *addLayer, reduceMean2, eps, false )
			|| graph.GetInputCount( *reduceMean2.Layer ) != 1 || !isValidDataLayer( *eps.Layer, CT_Float, /*size*/1 ) )
		{
			continue; // fail this Fusion
		}

		CLayoutChange layoutChangeBeforePooling;
		auto* sqrLayer = dynamic_cast<CPowerLayer*>(
			selectLayoutChange( *reduceMean2.Layer, 0, layoutChangeBeforePooling ).Layer );
		if( sqrLayer == nullptr || !isValidPowerLayer( *sqrLayer, 2.f ) ||
			!layoutChangeBeforePooling.IsInversionOf( layoutChangeAfterPooling ) )
		{
			continue; // fail this Fusion
		}

		COnnxEltwiseLayer* subLayer = nullptr;
		// try to skip CAST layer in operand (1)
		CCastLayer* unusedCastLayer = graph.SelectTheOnlyConnectedOutput<CCastLayer>( *sqrLayer, false );
		if( unusedCastLayer != nullptr ) { // success to find the CAST layer as operand (1)
			if( !isValidCastLayer( *unusedCastLayer ) ) {
				continue; // fail this Fusion
			}
			subLayer = graph.GetConnectedOutput<COnnxEltwiseLayer>( *unusedCastLayer, /*inputIndex*/0 ).Layer;
		} else { // fail to find the CAST layer as operand (1)
			subLayer = graph.GetConnectedOutput<COnnxEltwiseLayer>( *sqrLayer, /*inputIndex*/0 ).Layer;
		}
		if( subLayer == nullptr || !isValidArithmeticLayer( *subLayer, COnnxEltwiseLayer::TOperation::Sub ) ) {
			continue; // fail this Fusion
		}

		CBaseLayer* inputNormLayerX = nullptr;
		auto* transform2Layer = graph.GetConnectedOutput<COnnxTransformHelper>( *subLayer, /*inputIndex*/0 ).Layer;
		if( transform2Layer == nullptr
			|| isValidTransformLayer( *transform2Layer, /*prevTransform*/transform4.Layer, /*opposite*/false, objTransform ) )
		{
			transform2Layer = graph.GetConnectedOutput<COnnxTransformHelper>( *subLayer, /*inputIndex*/1 ).Layer;
			inputNormLayerX = graph.GetConnectedOutput<CBaseLayer>( *subLayer, /*inputIndex*/0 ).Layer;
			if( transform2Layer == nullptr
				|| !isValidTransformLayer( *transform2Layer, /*prevTransform*/transform4.Layer, /*opposite*/false, objTransform ) )
			{
				continue; // fail this Fusion
			}
		} else {
			inputNormLayerX = graph.GetConnectedOutput<CBaseLayer>( *subLayer, /*inputIndex*/1 ).Layer;
		}
		graph.SelectLayer( *transform2Layer );

		//CGlobalMeanPoolingLayer* reduceMeanLayer = graph.SelectTheOnlyConnectedOutput<CGlobalMeanPoolingLayer>( *transform2Layer, /*checkOutOfSelectionLinks*/false );
		//if( reduceMeanLayer == nullptr
		//	|| graph.GetInputCount( *reduceMeanLayer ) != 1 )
		//{
		//	continue; // fail this Fusion
		//}

		//auto* transform1Layer = graph.SelectTheOnlyConnectedOutput<COnnxTransformHelper>( *reduceMeanLayer, /*checkOutOfSelectionLinks*/false );
		//if( transform1Layer == nullptr
		//	|| !isValidTransformLayer( *transform1Layer, /*prevTransform*/transform4.Layer, /*opposite*/true, objTransform ) )
		//{
		//	continue; // fail this Fusion
		//}

		//// Handle cyclic edges check (1)
		//if( sub2.Layer != subLayer ) { // Duplicated sub-layers exported from older version of PyTorch
		//	NeoAssert( graph.IsLayerSelected( *subLayer ) == false );
		//	graph.SelectLayer( *subLayer );
		//	auto* in1 = graph.GetConnectedOutput<CGlobalMeanPoolingLayer>( *sub2.Layer, /*inputIndex*/0 ).Layer;
		//	auto* in2 = graph.GetConnectedOutput<CBaseLayer>( *sub2.Layer, /*inputIndex*/1 ).Layer;
		//	if( in1 != reduceMeanLayer || in2 != inputNormLayerX ) {
		//		continue; // fail this Fusion
		//	}
		//}
		//// Handle cyclic edges check (2)
		//auto* inputReduceMeanLayer = graph.GetConnectedOutput( *transform1Layer, /*inputIndex*/0 ).Layer;
		//if( inputReduceMeanLayer != inputNormLayerX ) {
		//	CBaseLayer* transformLayer = transform1Layer;
		//	while( ( transformLayer = graph.SelectTheOnlyConnectedOutput<COnnxTransformHelper>( *transformLayer ) ) != nullptr ) {
		//		inputReduceMeanLayer = graph.GetConnectedOutput( *transformLayer, /*inputIndex*/0 ).Layer;
		//	}
		//	if ( inputReduceMeanLayer != inputNormLayerX ) {
		//		continue; // fail this Fusion
		//	}
		//}
		//// Current Fusion succeed!

		//const auto& biasBlob = bias.Layer->GetBlob();
		//const auto& scaleBlob = scale.Layer->GetBlob();
		//const auto& epsBlob = eps.Layer->GetBlob();

		//CPtr<CObjectNormalizationLayer> normLayer{ new CObjectNormalizationLayer( graph.MathEngine() ) };
		//normLayer->SetName( graph.GetUniqueName( CString( fusionNamePrefix ) + "ObjNorm_" ) );
		//normLayer->SetEpsilon( epsBlob->GetData().GetValue() );
		//graph.AddLayer( *normLayer );

		//CBaseLayer* newNormLayer = nullptr;
		//if( objTransform ) {
		//	int rule[BD_Count]{ -1, -1, -1, -1, -1, -1, -1 };
		//	getTransformRule( *transform4.Layer, /*opposite*/false, rule );

		//	for( int index = 0; index < BD_Count; ++index ) {
		//		if( !isEmptyBlobDim( index, rule[index] )  ) {
		//			biasBlob->GetTransposed( index, rule[index] );
		//			scaleBlob->GetTransposed( index, rule[index] );
		//		}
		//	}
		//	normLayer->SetBias( biasBlob );
		//	normLayer->SetScale( scaleBlob );

		//	// Layer ObjectNormalization should have some number of objects to reduction ( ObjectCount > 1 && ObjectSize >= 1 ).
		//	// Mostly in given ANNs X layer (input of ObjNorm) has Height > 1, so it used to increase the ObjectCount.

		//	CPtr<COnnxTransformHelper> layerTransformBefore{ new COnnxTransformHelper( graph.MathEngine() ) };
		//	layerTransformBefore->SetName( graph.GetUniqueName( CString( fusionNamePrefix ) + "TransformBefore_" ) );
		//	graph.AddLayer( *layerTransformBefore );

		//	for( int index = 0; index < BD_Count; ++index ) {
		//		if( isValidBlobDim( rule[index] ) ) {
		//			layerTransformBefore->SetRule( TBlobDim( index ), TBlobDim( rule[index] ) );
		//		}
		//	}

		//	CPtr<COnnxTransformHelper> layerTransformAfter{ new COnnxTransformHelper( graph.MathEngine() ) };
		//	layerTransformAfter->SetName( graph.GetUniqueName( CString( fusionNamePrefix ) + "TransformAfter_" ) );
		//	graph.AddLayer( *layerTransformAfter );

		//	int ruleOpposite[BD_Count]{ -1, -1, -1, -1, -1, -1, -1 };
		//	getTransformRule( *transform4.Layer, /*opposite*/true, ruleOpposite );
		//	for( int index = 0; index < BD_Count; ++index ) {
		//		if( isValidBlobDim( ruleOpposite[index] ) ) {
		//			layerTransformAfter->SetRule( TBlobDim( index ), TBlobDim( ruleOpposite[index] ) );
		//		}
		//	}

		//	graph.Connect( *layerTransformBefore, /*inputIndex*/0, *inputNormLayerX, /*outputIndex*/0 );
		//	graph.Connect( *normLayer, /*inputIndex*/0, *layerTransformBefore, /*outputIndex*/0 );
		//	graph.Connect( *layerTransformAfter, /*inputIndex*/0, *normLayer, /*outputIndex*/0 );
		//	newNormLayer = layerTransformAfter;
		//} else {
		//	normLayer->SetBias( biasBlob );
		//	normLayer->SetScale( scaleBlob );

		//	graph.Connect( *normLayer, /*inputIndex*/0, *inputNormLayerX, /*outputIndex*/0 );
		//	newNormLayer = normLayer;
		//};

		//// Search for the output-layers of addLayerLast for new added ObjNorm layer
		//graph.SwitchOutputs( *addLayerLast, /*outputIndex*/0, *newNormLayer, /*outputIndex*/0 );

		//// All selected layers would be removed from the dnn
		//graph.DeleteSelectedLayers();
	} //for layers
}

} // namespace optimization

} // namespace NeoOnnx

